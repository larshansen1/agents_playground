"""Tests package."""
"""Pytest configuration and fixtures."""

import os

# Set environment variables for testing before importing app modules
os.environ["POSTGRES_PASSWORD"] = "test"
os.environ["OPENROUTER_API_KEY"] = "test"

import asyncio
import sqlite3
import uuid
from collections.abc import AsyncGenerator, Generator

import pytest
import pytest_asyncio
from httpx import ASGITransport, AsyncClient
from sqlalchemy import JSON, Column, DateTime, String, Text, func
from sqlalchemy.dialects import postgresql
from sqlalchemy.ext.asyncio import AsyncSession, async_sessionmaker, create_async_engine
from sqlalchemy.orm import declarative_base
from sqlalchemy.pool import StaticPool

from app.database import get_db
from app.main import app

# Test database URL (in-memory SQLite for speed)
TEST_DATABASE_URL = "sqlite+aiosqlite:///:memory:"

# Create test-specific base
TestBase = declarative_base()


class TestTask(TestBase):
    """Test Task model compatible with SQLite."""

    __tablename__ = "tasks"

    id = Column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    type = Column(String, nullable=False)
    status = Column(String, nullable=False, default="pending")
    input = Column(JSON, nullable=False)
    output = Column(JSON)
    error = Column(Text)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    # Cost tracking fields
    user_id_hash = Column(String(64))
    tenant_id = Column(String(100))
    model_used = Column(String(100))
    input_tokens = Column(postgresql.INTEGER)
    output_tokens = Column(postgresql.INTEGER)
    total_cost = Column(postgresql.NUMERIC(10, 6))
    generation_id = Column(String(100))

    # Lease-based task acquisition fields
    locked_at = Column(DateTime(timezone=True))
    locked_by = Column(String(100))
    lease_timeout = Column(DateTime(timezone=True))
    try_count = Column(postgresql.INTEGER, default=0)
    max_tries = Column(postgresql.INTEGER, default=3)


class TestSubtask(TestBase):
    """Test Subtask model compatible with SQLite."""

    __tablename__ = "subtasks"

    id = Column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    parent_task_id = Column(String(36), nullable=False)
    agent_type = Column(String, nullable=False)
    iteration = Column(postgresql.INTEGER, nullable=False, default=1)
    status = Column(String, nullable=False, default="pending")
    input = Column(JSON, nullable=False)
    output = Column(JSON)
    error = Column(Text)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    # Cost tracking fields
    user_id_hash = Column(String(64))
    tenant_id = Column(String(100))
    model_used = Column(String(100))
    input_tokens = Column(postgresql.INTEGER)
    output_tokens = Column(postgresql.INTEGER)
    total_cost = Column(postgresql.NUMERIC(10, 6))
    generation_id = Column(String(100))

    # Lease-based task acquisition fields
    locked_at = Column(DateTime(timezone=True))
    locked_by = Column(String(100))
    lease_timeout = Column(DateTime(timezone=True))
    try_count = Column(postgresql.INTEGER, default=0)
    max_tries = Column(postgresql.INTEGER, default=3)


class TestWorkflowState(TestBase):
    """Test WorkflowState model compatible with SQLite."""

    __tablename__ = "workflow_state"

    id = Column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    parent_task_id = Column(String(36), nullable=False, unique=True)
    workflow_type = Column(String, nullable=False)
    current_iteration = Column(postgresql.INTEGER, nullable=False, default=1)
    max_iterations = Column(postgresql.INTEGER, nullable=False, default=3)
    current_state = Column(String, nullable=False)
    state_data = Column(JSON)
    tenant_id = Column(String(100))
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now())


class TestAuditLog(TestBase):
    """Test AuditLog model compatible with SQLite."""

    __tablename__ = "audit_logs"

    id = Column(String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    timestamp = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
    event_type = Column(String, nullable=False)
    user_id_hash = Column(String(64))
    tenant_id = Column(String(100))
    resource_id = Column(String(36))
    metadata_ = Column("metadata", JSON)


@pytest.fixture(scope="session")
def event_loop() -> Generator:
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest_asyncio.fixture
async def async_engine():
    """Create a test database engine."""
    engine = create_async_engine(
        TEST_DATABASE_URL,
        connect_args={"check_same_thread": False},
        poolclass=StaticPool,
        echo=False,
    )

    async with engine.begin() as conn:
        await conn.run_sync(TestBase.metadata.create_all)

    yield engine

    async with engine.begin() as conn:
        await conn.run_sync(TestBase.metadata.drop_all)

    await engine.dispose()


@pytest_asyncio.fixture
async def async_session(async_engine) -> AsyncGenerator[AsyncSession, None]:
    """Create async database session for tests."""
    async_session_maker = async_sessionmaker(
        async_engine,
        class_=AsyncSession,
        expire_on_commit=False,
    )

    async with async_session_maker() as session:
        yield session


@pytest_asyncio.fixture
async def client(async_session: AsyncSession) -> AsyncGenerator[AsyncClient, None]:
    """Create async HTTP client for testing."""

    async def override_get_db():
        yield async_session

    app.dependency_overrides[get_db] = override_get_db

    async with AsyncClient(transport=ASGITransport(app=app), base_url="http://test") as ac:
        yield ac

    app.dependency_overrides.clear()


@pytest.fixture
def sample_task_data():
    """Sample task creation data."""
    return {
        "type": "summarize_document",
        "input": {"text": "Sample document for testing purposes."},
    }


@pytest.fixture
def sample_task_update():
    """Sample task update data."""
    return {
        "status": "done",
        "output": {"summary": "Test summary", "key_points": ["Point 1", "Point 2"]},
    }


# Synchronous DB fixtures for workflow integration tests


class SyncDBConnection:
    """SQLite connection wrapper that mimics psycopg2 interface for testing."""

    def __init__(self, engine):
        self.engine = engine
        self._connection = None
        self._in_transaction = False

    def __enter__(self):
        # Create a sync connection from the async engine
        # We use a simple SQLite connection for testing
        db_path = ":memory:"
        self._connection = sqlite3.connect(db_path, check_same_thread=False)
        self._connection.row_factory = sqlite3.Row
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self._connection:
            self._connection.close()

    def cursor(self):
        """Get a cursor (RealDictCursor-like behavior via Row)."""
        return self._connection.cursor()

    def commit(self):
        """Commit the transaction."""
        if self._connection:
            self._connection.commit()

    def rollback(self):
        """Rollback the transaction."""
        if self._connection:
            self._connection.rollback()

    def execute(self, query, params=None):
        """Execute a query directly on connection."""
        cursor = self.cursor()
        if params:
            cursor.execute(query, params)
        else:
            cursor.execute(query)
        return cursor


@pytest.fixture
def sync_db_engine(async_engine):
    """Create a synchronous database engine for workflow tests."""
    # Create in-memory SQLite connection
    conn = sqlite3.connect(":memory:", check_same_thread=False)
    conn.row_factory = sqlite3.Row

    # Create tables (simplified schema for testing)
    cursor = conn.cursor()

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS tasks (
            id TEXT PRIMARY KEY,
            type TEXT NOT NULL,
            status TEXT NOT NULL DEFAULT 'pending',
            input TEXT,
            output TEXT,
            error TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            user_id_hash TEXT,
            tenant_id TEXT,
            model_used TEXT,
            input_tokens INTEGER DEFAULT 0,
            output_tokens INTEGER DEFAULT 0,
            total_cost REAL DEFAULT 0.0,
            generation_id TEXT,
            locked_at TIMESTAMP,
            locked_by TEXT,
            lease_timeout TIMESTAMP,
            try_count INTEGER DEFAULT 0,
            max_tries INTEGER DEFAULT 3
        )
    """)

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS subtasks (
            id TEXT PRIMARY KEY,
            parent_task_id TEXT NOT NULL,
            agent_type TEXT NOT NULL,
            iteration INTEGER NOT NULL DEFAULT 1,
            status TEXT NOT NULL DEFAULT 'pending',
            input TEXT,
            output TEXT,
            error TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            user_id_hash TEXT,
            tenant_id TEXT,
            model_used TEXT,
            input_tokens INTEGER DEFAULT 0,
            output_tokens INTEGER DEFAULT 0,
            total_cost REAL DEFAULT 0.0,
            generation_id TEXT,
            locked_at TIMESTAMP,
            locked_by TEXT,
            lease_timeout TIMESTAMP,
            try_count INTEGER DEFAULT 0,
            max_tries INTEGER DEFAULT 3
        )
    """)

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS workflow_state (
            id TEXT PRIMARY KEY,
            parent_task_id TEXT NOT NULL UNIQUE,
            workflow_type TEXT NOT NULL,
            current_iteration INTEGER NOT NULL DEFAULT 1,
            max_iterations INTEGER NOT NULL DEFAULT 3,
            current_state TEXT NOT NULL,
            state_data TEXT,
            tenant_id TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)

    cursor.execute("""
        CREATE TABLE IF NOT EXISTS audit_logs (
            id TEXT PRIMARY KEY,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            event_type TEXT NOT NULL,
            user_id_hash TEXT,
            tenant_id TEXT,
            resource_id TEXT,
            metadata TEXT
        )
    """)

    conn.commit()

    yield conn

    conn.close()


# Mock agents for workflow integration tests


class MockResearchAgent:
    """Mock research agent for testing."""

    def __init__(self, predetermined_output=None, should_fail=False):
        self.agent_type = "research"
        self.predetermined_output = predetermined_output
        self.should_fail = should_fail
        self.call_count = 0

    def execute(self, input_data, user_id_hash=None):  # noqa: ARG002
        """Execute mock research task."""
        self.call_count += 1

        if self.should_fail:
            msg = "Mock research agent failed"
            raise RuntimeError(msg)

        # Default output if none provided
        if self.predetermined_output:
            output = self.predetermined_output
        else:
            iteration_suffix = f" (iteration {self.call_count})"
            topic = input_data.get("topic", "default topic")
            output = {
                "findings": f"Research findings for {topic}{iteration_suffix}",
                "sources": ["source1.com", "source2.com"],
                "key_insights": [f"Insight {self.call_count}"],
                "confidence_level": "high",
            }

        return {
            "output": output,
            "usage": {
                "model_used": "mock-research-model",
                "input_tokens": 100,
                "output_tokens": 200,
                "total_cost": 0.0001,
                "generation_id": f"mock-gen-{self.call_count}",
            },
        }


class MockAssessmentAgent:
    """Mock assessment agent for testing."""

    def __init__(self, predetermined_output=None, should_fail=False, approve_on_iteration=None):
        self.agent_type = "assessment"
        self.predetermined_output = predetermined_output
        self.should_fail = should_fail
        self.approve_on_iteration = approve_on_iteration
        self.call_count = 0

    def execute(self, input_data, user_id_hash=None):  # noqa: ARG002
        """Execute mock assessment task."""
        self.call_count += 1

        if self.should_fail:
            msg = "Mock assessment agent failed"
            raise RuntimeError(msg)

        # Default output if none provided
        if self.predetermined_output:
            output = self.predetermined_output
        else:
            # Approve based on iteration count
            approved = False
            if self.approve_on_iteration is not None:
                approved = self.call_count >= self.approve_on_iteration
            else:
                # Default: approve on first call
                approved = True

            output = {
                "approved": approved,
                "quality_score": 85 if approved else 60,
                "feedback": "Good work"
                if approved
                else f"Needs improvement (assessment {self.call_count})",
                "suggestions": [] if approved else ["Add more detail", "Cite sources"],
            }

        return {
            "output": output,
            "usage": {
                "model_used": "mock-assessment-model",
                "input_tokens": 150,
                "output_tokens": 100,
                "total_cost": 0.00008,
                "generation_id": f"mock-assess-gen-{self.call_count}",
            },
        }


@pytest.fixture
def mock_research_agent():
    """Fixture for mock research agent."""
    return MockResearchAgent()


@pytest.fixture
def mock_assessment_agent():
    """Fixture for mock assessment agent."""
    return MockAssessmentAgent()


@pytest.fixture
def mock_agent_registry():
    """Fixture that provides a mock agent registry."""

    def _get_mock_agent(agent_type, **kwargs):
        if agent_type == "research":
            return MockResearchAgent(**kwargs)
        if agent_type == "assessment":
            return MockAssessmentAgent(**kwargs)
        msg = f"Unknown mock agent type: {agent_type}"
        raise ValueError(msg)

    return _get_mock_agent
"""Tests for the Agent Registry.

This test suite covers all functional requirements for Phase 1:
- FR1: Agent Registration
- FR2: Agent Instantiation
- FR3: Fresh Instance Creation
- FR4: Agent Discovery
"""

import pytest

from app.agents.assessment_agent import AssessmentAgent
from app.agents.base import Agent
from app.agents.registry import AgentMetadata, AgentRegistry
from app.agents.research_agent import ResearchAgent


class MockAgent(Agent):
    """Mock agent for testing."""

    def __init__(self):
        super().__init__(agent_type="mock")

    def execute(self, _input_data: dict, _user_id_hash: str | None = None) -> dict:
        """Mock execute method."""
        return {"output": {}, "usage": {}}


class InvalidAgent:
    """Invalid agent that doesn't inherit from Agent."""



@pytest.fixture
def registry():
    """Fresh registry instance for each test."""
    return AgentRegistry()


@pytest.fixture
def mock_agent_class():
    """Mock agent class for testing."""
    return MockAgent


# ============================================================================
# Registration Tests (FR1)
# ============================================================================


def test_register_agent_success(registry, mock_agent_class):
    """Test successful agent registration."""
    registry.register(
        "mock",
        mock_agent_class,
        config={"model": "gpt-4-turbo"},
        tools=["web_search"],
        description="Mock agent for testing",
    )

    assert registry.has("mock")
    assert "mock" in registry.list_all()


def test_register_duplicate_raises_error(registry, mock_agent_class):
    """Test that duplicate registration raises ValueError."""
    registry.register("mock", mock_agent_class)

    with pytest.raises(ValueError, match="already registered"):
        registry.register("mock", mock_agent_class)


def test_register_with_default_values(registry, mock_agent_class):
    """Test registration with default values for optional parameters."""
    registry.register("mock", mock_agent_class)

    metadata = registry.get_metadata("mock")
    assert metadata.config == {}
    assert metadata.tools == []
    assert metadata.description == ""


def test_register_with_all_parameters(registry, mock_agent_class):
    """Test registration with all parameters provided."""
    registry.register(
        agent_type="mock",
        agent_class=mock_agent_class,
        config={"model": "gpt-4-turbo", "temperature": 0.7},
        tools=["web_search", "document_reader"],
        description="Full registration test",
    )

    metadata = registry.get_metadata("mock")
    assert metadata.config == {"model": "gpt-4-turbo", "temperature": 0.7}
    assert metadata.tools == ["web_search", "document_reader"]
    assert metadata.description == "Full registration test"


def test_register_invalid_agent_class_raises_error(registry):
    """Test that registering invalid agent class raises TypeError."""
    with pytest.raises(TypeError, match="must inherit from Agent base class"):
        registry.register("invalid", InvalidAgent)


def test_register_empty_agent_type_raises_error(registry, mock_agent_class):
    """Test that empty agent_type raises ValueError."""
    with pytest.raises(ValueError, match="cannot be empty"):
        registry.register("", mock_agent_class)


# ============================================================================
# Instantiation Tests (FR2)
# ============================================================================


def test_get_agent_returns_instance(registry, mock_agent_class):
    """Test that get() returns an agent instance."""
    registry.register("mock", mock_agent_class)

    agent = registry.get("mock")

    assert isinstance(agent, Agent)
    assert isinstance(agent, mock_agent_class)


def test_get_agent_returns_singleton(registry, mock_agent_class):
    """Test that get() returns the same instance on repeated calls."""
    registry.register("mock", mock_agent_class)

    agent1 = registry.get("mock")
    agent2 = registry.get("mock")

    assert agent1 is agent2  # Same object


def test_get_unknown_agent_raises_error(registry):
    """Test that getting unknown agent raises ValueError with helpful message."""
    registry.register("research", ResearchAgent)

    with pytest.raises(ValueError, match="Unknown agent type: 'nonexistent'"):
        registry.get("nonexistent")

    # Verify error message includes available agents
    with pytest.raises(ValueError, match="Available agents") as exc_info:
        registry.get("nonexistent")
    assert "research" in str(exc_info.value)


def test_create_new_returns_fresh_instance(registry, mock_agent_class):
    """Test that create_new() returns a new instance."""
    registry.register("mock", mock_agent_class)

    agent1 = registry.get("mock")  # Singleton
    agent2 = registry.create_new("mock")  # Fresh instance

    assert agent1 is not agent2  # Different objects
    assert isinstance(agent2, mock_agent_class)


def test_create_new_with_config_override(registry, mock_agent_class):
    """Test that create_new() accepts config overrides.

    Note: With Option 1 approach, config overrides are stored but not
    passed to no-arg constructors. This test verifies the interface works.
    """
    registry.register(
        "mock",
        mock_agent_class,
        config={"model": "gpt-4-turbo", "temperature": 0.7},
    )

    # Create instance with override (stored in metadata but not used with no-arg constructor)
    agent = registry.create_new("mock", temperature=0.9)

    assert isinstance(agent, mock_agent_class)


def test_create_new_unknown_agent_raises_error(registry):
    """Test that create_new() raises error for unknown agent."""
    with pytest.raises(ValueError, match="Unknown agent type"):
        registry.create_new("nonexistent")


# ============================================================================
# Discovery Tests (FR4)
# ============================================================================


def test_list_all_agents(registry):
    """Test that list_all() returns all registered agent types."""
    assert registry.list_all() == []

    registry.register("research", ResearchAgent)
    registry.register("assessment", AssessmentAgent)

    agents = registry.list_all()
    assert len(agents) == 2
    assert "research" in agents
    assert "assessment" in agents


def test_has_agent(registry, mock_agent_class):
    """Test that has() correctly checks agent existence."""
    assert not registry.has("mock")

    registry.register("mock", mock_agent_class)

    assert registry.has("mock")
    assert not registry.has("nonexistent")


def test_get_metadata(registry, mock_agent_class):
    """Test that get_metadata() returns correct metadata."""
    registry.register(
        "mock",
        mock_agent_class,
        config={"model": "gpt-4-turbo"},
        tools=["web_search"],
        description="Test agent",
    )

    metadata = registry.get_metadata("mock")

    assert isinstance(metadata, AgentMetadata)
    assert metadata.agent_class == mock_agent_class
    assert metadata.config == {"model": "gpt-4-turbo"}
    assert "web_search" in metadata.tools
    assert metadata.description == "Test agent"


def test_get_metadata_unknown_agent_raises_error(registry):
    """Test that get_metadata() raises error for unknown agent."""
    with pytest.raises(ValueError, match="Unknown agent type"):
        registry.get_metadata("nonexistent")


# ============================================================================
# Error Handling Tests
# ============================================================================


def test_error_message_includes_available_agents(registry):
    """Test that error messages include list of available agents."""
    registry.register("research", ResearchAgent)
    registry.register("assessment", AssessmentAgent)

    with pytest.raises(ValueError, match="Unknown agent type: 'typo_research'") as exc_info:
        registry.get("typo_research")

    error_msg = str(exc_info.value)
    assert "Available agents:" in error_msg
    assert "'research'" in str(exc_info.value)
    assert "'assessment'" in error_msg


def test_duplicate_registration_error_message(registry, mock_agent_class):
    """Test that duplicate registration error includes existing details."""
    registry.register("mock", mock_agent_class, tools=["tool1", "tool2"])

    with pytest.raises(ValueError, match="already registered") as exc_info:
        registry.register("mock", mock_agent_class)

    error_msg = str(exc_info.value)
    assert "'mock' is already registered" in error_msg
    assert "MockAgent" in error_msg
    assert "tools=" in error_msg


def test_metadata_for_unknown_agent_raises_error(registry):
    """Test that accessing metadata for unknown agent raises clear error."""
    with pytest.raises(ValueError, match="Unknown agent type") as exc_info:
        registry.get_metadata("unknown")

    assert "Unknown agent type" in str(exc_info.value)
    assert "Available agents" in str(exc_info.value)


# ============================================================================
# Integration Tests
# ============================================================================


def test_full_workflow_with_real_agents(registry):
    """Test complete workflow with real Research and Assessment agents."""
    # Register agents
    registry.register(
        "research",
        ResearchAgent,
        config={"model": "gpt-4-turbo", "temperature": 0.7},
        tools=["web_search"],
        description="Gathers information from web sources",
    )

    registry.register(
        "assessment",
        AssessmentAgent,
        config={"model": "gpt-4-turbo", "temperature": 0.3},
        tools=["fact_checker"],
        description="Assesses research quality",
    )

    # List all
    assert len(registry.list_all()) == 2

    # Get singleton
    agent1 = registry.get("research")
    agent2 = registry.get("research")
    assert agent1 is agent2  # Same instance

    # Create new
    agent3 = registry.create_new("research", temperature=0.9)
    assert agent3 is not agent1  # Different instance

    # Get metadata
    metadata = registry.get_metadata("research")
    assert "web_search" in metadata.tools
    assert metadata.description == "Gathers information from web sources"


# ============================================================================
# Thread Safety Tests (Optional - Basic verification)
# ============================================================================


def test_multiple_registration_calls(registry, mock_agent_class):
    """Test that registration is idempotent when expected."""
    # First registration should succeed
    registry.register("mock", mock_agent_class)

    # Second should fail with clear error
    with pytest.raises(ValueError, match="already registered"):
        registry.register("mock", mock_agent_class)

    # But registry should still work
    assert registry.has("mock")
    assert registry.get("mock") is not None
"""Unit tests for agents with mocked LLM calls."""

import json
from unittest.mock import MagicMock, patch

import pytest

from app.agents.assessment_agent import AssessmentAgent
from app.agents.research_agent import ResearchAgent
from app.tasks import calculate_cost


class TestResearchAgent:
    """Tests for ResearchAgent execution."""

    @pytest.fixture
    def mock_openai_client(self):
        """Mock OpenAI client."""
        with patch("app.agents.research_agent.client") as mock_client:
            yield mock_client

    def test_prompt_construction_initial(self, mock_openai_client):
        """Verify prompt construction for initial research."""
        agent = ResearchAgent()
        input_data = {"topic": "Quantum Computing"}

        # Setup mock response
        mock_response = MagicMock()
        mock_response.choices[0].message.content = json.dumps({"findings": "test"})
        mock_response.usage.prompt_tokens = 10
        mock_response.usage.completion_tokens = 20
        mock_openai_client.chat.completions.create.return_value = mock_response

        agent.execute(input_data)

        # Verify call arguments
        call_args = mock_openai_client.chat.completions.create.call_args
        assert call_args is not None
        messages = call_args.kwargs["messages"]

        # Check system prompt
        assert messages[0]["role"] == "system"
        assert "research" in messages[0]["content"].lower()

        # Check user content
        assert messages[1]["role"] == "user"
        content = json.loads(messages[1]["content"])
        assert content["topic"] == "Quantum Computing"
        assert "previous_feedback" not in content

    def test_prompt_construction_revision(self, mock_openai_client):
        """Verify prompt construction for revision iteration."""
        agent = ResearchAgent()
        input_data = {
            "topic": "Quantum Computing",
            "previous_feedback": "Add more details on qubits",
        }

        # Setup mock response
        mock_response = MagicMock()
        mock_response.choices[0].message.content = json.dumps({"findings": "test"})
        mock_openai_client.chat.completions.create.return_value = mock_response

        agent.execute(input_data)

        # Verify call arguments
        call_args = mock_openai_client.chat.completions.create.call_args
        messages = call_args.kwargs["messages"]

        # Check user content includes feedback
        content = json.loads(messages[1]["content"])
        assert content["topic"] == "Quantum Computing"
        assert content["previous_feedback"] == "Add more details on qubits"
        assert "note" in content  # Should have the revision note

    def test_response_parsing_valid_json(self, mock_openai_client):
        """Verify parsing of valid JSON response."""
        agent = ResearchAgent()
        expected_output = {
            "findings": "Quantum computers use qubits.",
            "sources": ["nature.com"],
            "key_insights": ["Superposition is key"],
            "confidence_level": "high",
        }

        # Setup mock response
        mock_response = MagicMock()
        mock_response.choices[0].message.content = json.dumps(expected_output)
        mock_openai_client.chat.completions.create.return_value = mock_response

        result = agent.execute({"topic": "test"})

        assert result["output"] == expected_output

    def test_response_parsing_invalid_json(self, mock_openai_client):
        """Verify fallback handling for invalid JSON response."""
        agent = ResearchAgent()
        raw_text = "Here are the findings: Quantum computers are fast."

        # Setup mock response with plain text
        mock_response = MagicMock()
        mock_response.choices[0].message.content = raw_text
        mock_openai_client.chat.completions.create.return_value = mock_response

        result = agent.execute({"topic": "test"})

        # Should wrap text in structured output
        assert result["output"]["findings"] == raw_text
        assert result["output"]["note"] == "Model returned plain text instead of JSON"
        assert result["output"]["confidence_level"] == "unknown"

    def test_cost_calculation(self, mock_openai_client):
        """Verify cost calculation based on token usage."""
        agent = ResearchAgent()

        # Setup mock usage
        input_tokens = 150
        output_tokens = 250
        mock_response = MagicMock()
        mock_response.choices[0].message.content = "{}"
        mock_response.usage.prompt_tokens = input_tokens
        mock_response.usage.completion_tokens = output_tokens
        mock_response.id = "gen-123"
        mock_openai_client.chat.completions.create.return_value = mock_response

        result = agent.execute({"topic": "test"})

        usage = result["usage"]
        assert usage["input_tokens"] == input_tokens
        assert usage["output_tokens"] == output_tokens
        assert usage["generation_id"] == "gen-123"

        # Verify cost matches utility calculation
        expected_cost = calculate_cost("gpt-4o-mini", input_tokens, output_tokens)
        assert usage["total_cost"] == expected_cost

    def test_user_id_header(self, mock_openai_client):
        """Verify X-User-ID header is passed."""
        agent = ResearchAgent()
        user_id_hash = "user-123-hash"

        mock_response = MagicMock()
        mock_response.choices[0].message.content = "{}"
        mock_openai_client.chat.completions.create.return_value = mock_response

        agent.execute({"topic": "test"}, user_id_hash=user_id_hash)

        call_args = mock_openai_client.chat.completions.create.call_args
        extra_headers = call_args.kwargs["extra_headers"]
        assert extra_headers["X-User-ID"] == user_id_hash


class TestAssessmentAgent:
    """Tests for AssessmentAgent execution."""

    @pytest.fixture
    def mock_openai_client(self):
        """Mock OpenAI client."""
        with patch("app.agents.assessment_agent.client") as mock_client:
            yield mock_client

    def test_prompt_construction(self, mock_openai_client):
        """Verify prompt construction for assessment."""
        agent = AssessmentAgent()
        input_data = {"research_findings": {"findings": "content"}, "original_topic": "Topic A"}

        mock_response = MagicMock()
        mock_response.choices[0].message.content = json.dumps({"approved": True})
        mock_openai_client.chat.completions.create.return_value = mock_response

        agent.execute(input_data)

        call_args = mock_openai_client.chat.completions.create.call_args
        messages = call_args.kwargs["messages"]

        # Check system prompt
        assert messages[0]["role"] == "system"
        assert "assess" in messages[0]["content"].lower()

        # Check user content
        content = json.loads(messages[1]["content"])
        assert content["original_topic"] == "Topic A"
        assert content["research_findings"] == {"findings": "content"}

    def test_response_parsing_approval(self, mock_openai_client):
        """Verify parsing of approval response."""
        agent = AssessmentAgent()
        expected_output = {"approved": True, "feedback": "Great job", "quality_score": 90}

        mock_response = MagicMock()
        mock_response.choices[0].message.content = json.dumps(expected_output)
        mock_openai_client.chat.completions.create.return_value = mock_response

        result = agent.execute({"research_findings": {}, "original_topic": "test"})

        assert result["output"] == expected_output
        assert result["output"]["approved"] is True

    def test_response_parsing_rejection(self, mock_openai_client):
        """Verify parsing of rejection response."""
        agent = AssessmentAgent()
        expected_output = {
            "approved": False,
            "feedback": "Missing sources",
            "areas_for_improvement": ["Add citations"],
        }

        mock_response = MagicMock()
        mock_response.choices[0].message.content = json.dumps(expected_output)
        mock_openai_client.chat.completions.create.return_value = mock_response

        result = agent.execute({"research_findings": {}, "original_topic": "test"})

        assert result["output"] == expected_output
        assert result["output"]["approved"] is False

    def test_error_handling_fallback(self, mock_openai_client):
        """Verify fallback when model returns invalid JSON."""
        agent = AssessmentAgent()

        mock_response = MagicMock()
        mock_response.choices[0].message.content = "I cannot approve this."
        mock_openai_client.chat.completions.create.return_value = mock_response

        result = agent.execute({"research_findings": {}, "original_topic": "test"})

        # Should default to rejected
        assert result["output"]["approved"] is False
        assert "Assessment agent error" in result["output"]["feedback"]
"""Tests for API endpoints."""

from uuid import UUID

import pytest
from httpx import AsyncClient


@pytest.mark.asyncio
class TestHealthEndpoint:
    """Tests for health check endpoint."""

    async def test_health_endpoint(self, client: AsyncClient):
        """Test health check returns 200 OK."""
        response = await client.get("/health")
        assert response.status_code == 200
        data = response.json()
        assert data["status"] == "healthy"
        assert "database" in data
        assert "websocket_connections" in data

    async def test_root_endpoint(self, client: AsyncClient):
        """Test root endpoint."""
        response = await client.get("/")
        assert response.status_code == 200
        data = response.json()
        assert data["status"] == "healthy"
        assert "message" in data


@pytest.mark.asyncio
class TestTaskEndpoints:
    """Tests for task CRUD endpoints."""

    async def test_create_task(self, client: AsyncClient, sample_task_data):
        """Test creating a new task."""
        response = await client.post("/tasks", json=sample_task_data)
        assert response.status_code == 201

        data = response.json()
        assert "id" in data
        assert UUID(data["id"])  # Valid UUID
        assert data["type"] == sample_task_data["type"]
        assert data["status"] == "pending"

        # Input might have trace context injected, so we check if original input is present
        response_input = data["input"].copy()
        response_input.pop("_trace_context", None)
        assert response_input == sample_task_data["input"]

        assert data["output"] is None
        assert data["error"] is None
        assert "created_at" in data
        assert "updated_at" in data

    async def test_create_task_invalid_data(self, client: AsyncClient):
        """Test creating task with invalid data."""
        response = await client.post("/tasks", json={"type": "test"})
        assert response.status_code == 422  # Validation error

    async def test_get_task(self, client: AsyncClient, sample_task_data):
        """Test retrieving a task by ID."""
        # Create task first
        create_response = await client.post("/tasks", json=sample_task_data)
        task_id = create_response.json()["id"]

        # Get task
        response = await client.get(f"/tasks/{task_id}")
        assert response.status_code == 200

        data = response.json()
        assert data["id"] == task_id
        assert data["type"] == sample_task_data["type"]

    async def test_get_nonexistent_task(self, client: AsyncClient):
        """Test getting a task that doesn't exist."""
        fake_id = "00000000-0000-0000-0000-000000000000"
        response = await client.get(f"/tasks/{fake_id}")
        assert response.status_code == 404

    async def test_list_tasks(self, client: AsyncClient, sample_task_data):
        """Test listing all tasks."""
        # Create multiple tasks
        await client.post("/tasks", json=sample_task_data)
        await client.post("/tasks", json={"type": "analyze_table", "input": {"table_name": "test"}})

        # List tasks
        response = await client.get("/tasks")
        assert response.status_code == 200

        data = response.json()
        assert isinstance(data, list)
        assert len(data) >= 2

    async def test_list_tasks_with_filter(self, client: AsyncClient, sample_task_data):
        """Test listing tasks with status filter."""
        await client.post("/tasks", json=sample_task_data)

        response = await client.get("/tasks?status_filter=pending")
        assert response.status_code == 200

        data = response.json()
        assert isinstance(data, list)
        for task in data:
            assert task["status"] == "pending"

    async def test_list_tasks_with_limit(self, client: AsyncClient, sample_task_data):
        """Test listing tasks with limit."""
        # Create multiple tasks
        for _ in range(5):
            await client.post("/tasks", json=sample_task_data)

        response = await client.get("/tasks?limit=3")
        assert response.status_code == 200

        data = response.json()
        assert len(data) <= 3

    async def test_update_task(self, client: AsyncClient, sample_task_data, sample_task_update):
        """Test updating a task."""
        # Create task
        create_response = await client.post("/tasks", json=sample_task_data)
        task_id = create_response.json()["id"]

        # Update task
        response = await client.patch(f"/tasks/{task_id}", json=sample_task_update)
        assert response.status_code == 200

        data = response.json()
        assert data["id"] == task_id
        assert data["status"] == sample_task_update["status"]
        assert data["output"] == sample_task_update["output"]

    async def test_update_task_status_only(self, client: AsyncClient, sample_task_data):
        """Test updating only task status."""
        # Create task
        create_response = await client.post("/tasks", json=sample_task_data)
        task_id = create_response.json()["id"]

        # Update status
        response = await client.patch(f"/tasks/{task_id}", json={"status": "running"})
        assert response.status_code == 200

        data = response.json()
        assert data["status"] == "running"
        assert data["output"] is None

    async def test_update_nonexistent_task(self, client: AsyncClient, sample_task_update):
        """Test updating a task that doesn't exist."""
        fake_id = "00000000-0000-0000-0000-000000000000"
        response = await client.patch(f"/tasks/{fake_id}", json=sample_task_update)
        assert response.status_code == 404


@pytest.mark.asyncio
class TestTaskLifecycle:
    """Integration tests for complete task lifecycle."""

    async def test_complete_task_lifecycle(self, client: AsyncClient):
        """Test complete task lifecycle: create -> get -> update -> get."""
        # 1. Create task
        create_data = {"type": "summarize_document", "input": {"text": "Test document"}}
        create_response = await client.post("/tasks", json=create_data)
        assert create_response.status_code == 201
        task_id = create_response.json()["id"]

        # 2. Get task (should be pending)
        get_response = await client.get(f"/tasks/{task_id}")
        assert get_response.status_code == 200
        assert get_response.json()["status"] == "pending"

        # 3. Update to running
        await client.patch(f"/tasks/{task_id}", json={"status": "running"})
        get_response = await client.get(f"/tasks/{task_id}")
        assert get_response.json()["status"] == "running"

        # 4. Update to done with output
        update_data = {"status": "done", "output": {"summary": "Task completed successfully"}}
        update_response = await client.patch(f"/tasks/{task_id}", json=update_data)
        assert update_response.status_code == 200

        # 5. Final get - verify completion
        final_response = await client.get(f"/tasks/{task_id}")
        final_data = final_response.json()
        assert final_data["status"] == "done"
        assert final_data["output"]["summary"] == "Task completed successfully"

    async def test_task_error_flow(self, client: AsyncClient):
        """Test task failure flow."""
        # Create task
        create_data = {"type": "analyze_table", "input": {"table_name": "test"}}
        create_response = await client.post("/tasks", json=create_data)
        task_id = create_response.json()["id"]

        # Update to running
        await client.patch(f"/tasks/{task_id}", json={"status": "running"})

        # Update to error
        error_data = {"status": "error", "error": "Table not found"}
        await client.patch(f"/tasks/{task_id}", json=error_data)

        # Verify error state
        response = await client.get(f"/tasks/{task_id}")
        data = response.json()
        assert data["status"] == "error"
        assert data["error"] == "Table not found"
        assert data["output"] is None
from unittest.mock import MagicMock

from app.audit import log_audit_event
from app.models import AuditLog


def test_log_audit_event_success():
    """Test successful audit log creation."""
    mock_db = MagicMock()

    log = log_audit_event(
        mock_db,
        event_type="test_event",
        resource_id="resource-123",
        user_id_hash="user-hash-123",
        meta={"key": "value"},
    )

    assert isinstance(log, AuditLog)
    assert log.event_type == "test_event"
    assert log.resource_id == "resource-123"
    assert log.user_id_hash == "user-hash-123"
    assert log.metadata_ == {"key": "value"}

    mock_db.add.assert_called_once_with(log)


def test_log_audit_event_failure():
    """Test audit log failure handling."""
    mock_db = MagicMock()
    mock_db.add.side_effect = Exception("DB Error")

    log = log_audit_event(mock_db, event_type="test_event")

    # Should return a dummy/empty AuditLog and not raise exception
    assert isinstance(log, AuditLog)
    assert log.event_type is None  # Default empty object
"""Tests for Agent Registry auto-discovery functionality."""

import pytest

from app.agents.registry import AgentRegistry


def test_discover_agents():
    """Test auto-discovery of agents from filesystem."""
    registry = AgentRegistry()
    discovered = registry.discover_agents("app/agents")

    # Should find ResearchAgent and AssessmentAgent
    assert "research" in discovered
    assert "assessment" in discovered
    assert registry.has("research")
    assert registry.has("assessment")


def test_discover_with_exclusions():
    """Test discovery with exclusion patterns."""
    registry = AgentRegistry()
    discovered = registry.discover_agents(
        "app/agents", exclude_patterns=["research_*", "base.py", "__*"]
    )

    # research_agent.py should be excluded
    assert "research" not in discovered
    # assessment_agent.py should be included
    assert "assessment" in discovered


def test_discover_returns_list():
    """Test that discover_agents returns list of discovered types."""
    registry = AgentRegistry()
    discovered = registry.discover_agents("app/agents")

    assert isinstance(discovered, list)
    assert len(discovered) > 0
    assert all(isinstance(agent_type, str) for agent_type in discovered)


def test_discover_skips_already_registered():
    """Test that discovery skips already-registered agents."""
    registry = AgentRegistry()

    # Manually register research agent
    from app.agents.research_agent import ResearchAgent

    registry.register("research", ResearchAgent)

    # Discover should skip it
    discovered = registry.discover_agents("app/agents")

    # research should not be in discovered list (already registered)
    assert "research" not in discovered
    # But assessment should be discovered
    assert "assessment" in discovered


def test_discover_nonexistent_path():
    """Test discovery with non-existent path."""
    registry = AgentRegistry()
    discovered = registry.discover_agents("nonexistent/path")

    # Should return empty list without crashing
    assert discovered == []


def test_discover_default_exclusions():
    """Test that default exclusions work (base.py, __*)."""
    registry = AgentRegistry()
    discovered = registry.discover_agents("app/agents")

    # Should not include 'base' (from base.py)
    assert "base" not in discovered
    # Should not include __init__ or __pycache__
    all_types = registry.list_all()
    assert not any(t.startswith("__") for t in all_types)


def test_combined_yaml_and_discovery(tmp_path):
    """Test using YAML loading and auto-discovery together."""
    # Create a YAML config
    yaml_content = """
agents:
  - name: yaml_agent
    class: app.agents.research_agent.ResearchAgent
    description: "From YAML"
"""
    yaml_file = tmp_path / "agents.yaml"
    yaml_file.write_text(yaml_content)

    registry = AgentRegistry()

    # Load from YAML
    registry.load_from_yaml(yaml_file)
    assert registry.has("yaml_agent")

    # Auto-discover (research should be skipped due to YAML registration)
    discovered = registry.discover_agents("app/agents")

    # assessment should be discovered
    assert registry.has("assessment")

    # All three methods should coexist
    from app.agents.assessment_agent import AssessmentAgent

    registry.register("manual", AssessmentAgent)

    assert registry.has("yaml_agent")  # from YAML
    assert registry.has("assessment")  # from discovery
    assert registry.has("manual")  # from programmatic


def test_discover_extracts_docstring():
    """Test that discovery extracts class docstring as description."""
    registry = AgentRegistry()
    registry.discover_agents("app/agents")

    metadata = registry.get_metadata("research")
    # ResearchAgent has a docstring
    assert metadata.description
    assert len(metadata.description) > 0
"""Tests for cost tracking functionality."""

import pytest
from httpx import AsyncClient
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.models import Task
from app.tasks import calculate_cost


class TestCostCalculation:
    """Unit tests for cost calculation logic."""

    def test_calculate_cost_gemini_flash(self):
        """Test cost calculation for Gemini 2.5 Flash with known token counts."""
        input_tokens = 1000
        output_tokens = 500

        cost = calculate_cost("google/gemini-2.5-flash", input_tokens, output_tokens)

        # Gemini 2.5 Flash: $0.075/1M input, $0.30/1M output
        expected_cost = (1000 * 0.075 / 1_000_000) + (500 * 0.30 / 1_000_000)
        assert cost == pytest.approx(expected_cost, abs=0.000001)
        assert cost == pytest.approx(0.000225, abs=0.000001)

    def test_calculate_cost_large_document(self):
        """Test cost calculation for large document."""
        input_tokens = 100_000
        output_tokens = 50_000

        cost = calculate_cost("google/gemini-2.5-flash", input_tokens, output_tokens)

        expected_cost = (100_000 * 0.075 / 1_000_000) + (50_000 * 0.30 / 1_000_000)
        assert cost == pytest.approx(expected_cost, abs=0.000001)
        assert cost == pytest.approx(0.0225, abs=0.000001)

    def test_calculate_cost_zero_tokens(self):
        """Test cost calculation with zero tokens."""
        cost = calculate_cost("google/gemini-2.5-flash", 0, 0)
        assert cost == 0.0

    def test_calculate_cost_only_input(self):
        """Test cost calculation with only input tokens."""
        cost = calculate_cost("google/gemini-2.5-flash", 1000, 0)
        expected_cost = 1000 * 0.075 / 1_000_000
        assert cost == pytest.approx(expected_cost, abs=0.000001)

    def test_calculate_cost_only_output(self):
        """Test cost calculation with only output tokens."""
        cost = calculate_cost("google/gemini-2.5-flash", 0, 1000)
        expected_cost = 1000 * 0.30 / 1_000_000
        assert cost == pytest.approx(expected_cost, abs=0.000001)

    def test_calculate_cost_precision(self):
        """Test cost calculation maintains precision for small amounts."""
        input_tokens = 10
        output_tokens = 10

        cost = calculate_cost("google/gemini-2.5-flash", input_tokens, output_tokens)

        # Should be able to track costs smaller than a cent
        assert cost > 0
        assert cost < 0.00001  # Less than $0.00001

    def test_calculate_cost_unknown_model_uses_default(self):
        """Test that unknown models use default pricing."""
        cost = calculate_cost("unknown/model", 1000, 500)

        # Should use default pricing: $0.15/$0.60
        expected_cost = (1000 * 0.15 / 1_000_000) + (500 * 0.60 / 1_000_000)
        assert cost == pytest.approx(expected_cost, abs=0.000001)


@pytest.mark.asyncio
class TestCostFieldPersistence:
    """Tests for cost field database persistence."""

    async def test_task_created_with_cost_fields_null(
        self, async_session: AsyncSession, sample_task_data
    ):
        """Test that new tasks have null cost fields initially."""
        task = Task(
            type=sample_task_data["type"], input=sample_task_data["input"], status="pending"
        )
        async_session.add(task)
        await async_session.commit()
        await async_session.refresh(task)

        assert task.user_id_hash is None
        assert task.model_used is None
        # SQLite may use 0 instead of NULL for integers
        assert task.input_tokens is None or task.input_tokens == 0
        assert task.output_tokens is None or task.output_tokens == 0
        assert task.total_cost is None or task.total_cost == 0
        assert task.generation_id is None

    async def test_task_cost_fields_persistence(
        self, async_session: AsyncSession, sample_task_data
    ):
        """Test that cost fields are persisted correctly."""
        task = Task(
            type=sample_task_data["type"],
            input=sample_task_data["input"],
            status="done",
            user_id_hash="test_user_hash_123",
            model_used="google/gemini-2.5-flash",
            input_tokens=840,
            output_tokens=448,
            total_cost=0.000197,
            generation_id="gen-test-123",
        )
        async_session.add(task)
        await async_session.commit()

        # Reload from database
        result = await async_session.execute(select(Task).where(Task.id == task.id))
        loaded_task = result.scalar_one()

        assert loaded_task.user_id_hash == "test_user_hash_123"
        assert loaded_task.model_used == "google/gemini-2.5-flash"
        assert loaded_task.input_tokens == 840
        assert loaded_task.output_tokens == 448
        assert float(loaded_task.total_cost) == pytest.approx(0.000197, abs=0.000001)
        assert loaded_task.generation_id == "gen-test-123"

    async def test_task_cost_update(self, async_session: AsyncSession, sample_task_data):
        """Test updating cost fields on existing task."""
        # Create task without cost data
        task = Task(
            type=sample_task_data["type"], input=sample_task_data["input"], status="pending"
        )
        async_session.add(task)
        await async_session.commit()
        task_id = task.id

        # Update with cost data
        task.status = "done"
        task.user_id_hash = "updated_user"
        task.model_used = "google/gemini-2.5-flash"
        task.input_tokens = 100
        task.output_tokens = 200
        task.total_cost = 0.000135
        await async_session.commit()

        # Verify update
        result = await async_session.execute(select(Task).where(Task.id == task_id))
        updated_task = result.scalar_one()

        assert updated_task.user_id_hash == "updated_user"
        assert updated_task.input_tokens == 100
        assert updated_task.output_tokens == 200


@pytest.mark.asyncio
class TestCostAggregationEndpoints:
    """Tests for cost aggregation API endpoints."""

    async def test_cost_by_user_single_task(self, client: AsyncClient, async_session: AsyncSession):
        """Test cost aggregation for single user with one task."""
        # Create task with cost data
        user_hash = "user_abc_123"
        task = Task(
            type="summarize_document",
            input={"text": "test"},
            status="done",
            user_id_hash=user_hash,
            model_used="google/gemini-2.5-flash",
            input_tokens=1000,
            output_tokens=500,
            total_cost=0.00045,
        )
        async_session.add(task)
        await async_session.commit()

        # Query cost endpoint
        response = await client.get(f"/tasks/costs/by-user/{user_hash}")
        assert response.status_code == 200

        data = response.json()
        assert data["total_tasks"] == 1
        assert data["total_input_tokens"] == 1000
        assert data["total_output_tokens"] == 500
        assert data["total_cost"] == pytest.approx(0.00045, abs=0.000001)

    async def test_cost_by_user_multiple_tasks(
        self, client: AsyncClient, async_session: AsyncSession
    ):
        """Test cost aggregation for user with multiple tasks."""
        user_hash = "user_multi_123"

        # Create multiple tasks
        tasks = [
            Task(
                type="summarize_document",
                input={"text": f"test{i}"},
                status="done",
                user_id_hash=user_hash,
                model_used="google/gemini-2.5-flash",
                input_tokens=100 * (i + 1),
                output_tokens=50 * (i + 1),
                total_cost=0.00001 * (i + 1),
            )
            for i in range(3)
        ]
        for task in tasks:
            async_session.add(task)
        await async_session.commit()

        # Query cost endpoint
        response = await client.get(f"/tasks/costs/by-user/{user_hash}")
        assert response.status_code == 200

        data = response.json()
        assert data["total_tasks"] == 3
        assert data["total_input_tokens"] == 100 + 200 + 300  # 600
        assert data["total_output_tokens"] == 50 + 100 + 150  # 300
        assert data["total_cost"] == pytest.approx(0.00006, abs=0.000001)

    async def test_cost_by_user_no_tasks(self, client: AsyncClient):
        """Test cost aggregation for user with no tasks."""
        response = await client.get("/tasks/costs/by-user/nonexistent_user")
        assert response.status_code == 200

        data = response.json()
        assert data["total_tasks"] == 0
        assert data["total_input_tokens"] == 0
        assert data["total_output_tokens"] == 0
        assert data["total_cost"] == 0.0

    async def test_cost_by_user_ignores_null_costs(
        self, client: AsyncClient, async_session: AsyncSession
    ):
        """Test that tasks without cost data are excluded from aggregation."""
        user_hash = "user_mixed_123"

        # Task with cost
        task1 = Task(
            type="summarize_document",
            input={"text": "test1"},
            status="done",
            user_id_hash=user_hash,
            input_tokens=100,
            output_tokens=50,
            total_cost=0.00001,
        )
        # Task without cost (pending)
        task2 = Task(
            type="summarize_document",
            input={"text": "test2"},
            status="pending",
            user_id_hash=user_hash,
        )
        async_session.add_all([task1, task2])
        await async_session.commit()

        response = await client.get(f"/tasks/costs/by-user/{user_hash}")
        data = response.json()

        # Note: In SQLite, unset integers become 0 (not NULL)
        # So both tasks will be counted, but only one has actual cost
        # In production PostgreSQL, NULL works as expected
        assert data["total_tasks"] == 2  # Both tasks counted in SQLite
        assert data["total_input_tokens"] == 100  # Only from task with cost
        assert data["total_cost"] == pytest.approx(0.00001, abs=0.000001)

    async def test_cost_summary_endpoint(self, client: AsyncClient, async_session: AsyncSession):
        """Test platform-wide cost summary endpoint."""
        # Create tasks for multiple users
        users_data = [
            ("user_1", 100, 50, 0.00001),
            ("user_1", 200, 100, 0.00002),
            ("user_2", 300, 150, 0.00003),
        ]

        for user_hash, input_tok, output_tok, cost in users_data:
            task = Task(
                type="summarize_document",
                input={"text": "test"},
                status="done",
                user_id_hash=user_hash,
                input_tokens=input_tok,
                output_tokens=output_tok,
                total_cost=cost,
            )
            async_session.add(task)
        await async_session.commit()

        response = await client.get("/tasks/costs/summary")
        assert response.status_code == 200

        data = response.json()
        assert data["unique_users"] == 2
        assert data["total_tasks"] == 3
        assert data["total_cost"] == pytest.approx(0.00006, abs=0.000001)
        assert data["avg_cost_per_task"] == pytest.approx(0.00002, abs=0.000001)

    async def test_cost_summary_empty_database(self, client: AsyncClient):
        """Test cost summary with no tasks."""
        response = await client.get("/tasks/costs/summary")
        assert response.status_code == 200

        data = response.json()
        assert data["unique_users"] == 0
        assert data["total_tasks"] == 0
        assert data["total_cost"] == 0.0
        assert data["avg_cost_per_task"] == 0.0


@pytest.mark.asyncio
class TestCostFieldsInTaskResponse:
    """Tests for cost fields in task API responses."""

    async def test_get_task_includes_cost_fields(
        self, client: AsyncClient, async_session: AsyncSession
    ):
        """Test that GET /tasks/{id} includes cost fields."""
        task = Task(
            type="summarize_document",
            input={"text": "test"},
            status="done",
            user_id_hash="test_user",
            model_used="google/gemini-2.5-flash",
            input_tokens=840,
            output_tokens=448,
            total_cost=0.000197,
            generation_id="gen-123",
        )
        async_session.add(task)
        await async_session.commit()

        response = await client.get(f"/tasks/{task.id}")
        assert response.status_code == 200

        data = response.json()
        assert data["user_id_hash"] == "test_user"
        assert data["model_used"] == "google/gemini-2.5-flash"
        assert data["input_tokens"] == 840
        assert data["output_tokens"] == 448
        assert data["total_cost"] == pytest.approx(0.000197, abs=0.000001)
        assert data["generation_id"] == "gen-123"

    async def test_list_tasks_includes_cost_fields(
        self, client: AsyncClient, async_session: AsyncSession
    ):
        """Test that GET /tasks includes cost fields."""
        task = Task(
            type="summarize_document",
            input={"text": "test"},
            status="done",
            user_id_hash="test_user",
            input_tokens=100,
            output_tokens=50,
            total_cost=0.00001,
        )
        async_session.add(task)
        await async_session.commit()

        response = await client.get("/tasks")
        assert response.status_code == 200

        tasks = response.json()
        assert len(tasks) >= 1

        # Find our task
        our_task = next(t for t in tasks if t["id"] == str(task.id))
        assert our_task["user_id_hash"] == "test_user"
        assert our_task["input_tokens"] == 100
        assert our_task["output_tokens"] == 50
        assert our_task["total_cost"] == pytest.approx(0.00001, abs=0.000001)


@pytest.mark.asyncio
class TestCostTrackingEndToEnd:
    """End-to-end integration tests for cost tracking."""

    async def test_complete_task_lifecycle_with_costs(self, client: AsyncClient):
        """Test complete task lifecycle including cost tracking."""
        # 1. Create task
        create_data = {
            "type": "summarize_document",
            "input": {"text": "Test document for cost tracking"},
        }
        create_response = await client.post("/tasks", json=create_data)
        assert create_response.status_code == 201
        task_id = create_response.json()["id"]

        # 2. Simulate worker completion with cost data
        # 2. Simulate worker completion with cost data
        # update_data would be used here in a real scenario

        # Note: In real scenario, worker would set these fields
        # For now, we'll verify schema accepts them via PATCH

        # 3. Get task and verify initial state (no cost data)
        get_response = await client.get(f"/tasks/{task_id}")
        task_data = get_response.json()
        assert task_data["total_cost"] is None or task_data["total_cost"] == 0.0
        assert task_data["input_tokens"] is None or task_data["input_tokens"] == 0

    async def test_multiple_users_cost_isolation(
        self, client: AsyncClient, async_session: AsyncSession
    ):
        """Test that costs are properly isolated per user."""
        # Create tasks for different users
        user1_hash = "user_1_hash"
        user2_hash = "user_2_hash"

        # User 1 tasks
        for i in range(2):
            task = Task(
                type="summarize_document",
                input={"text": f"user1_task{i}"},
                status="done",
                user_id_hash=user1_hash,
                input_tokens=100,
                output_tokens=50,
                total_cost=0.00001,
            )
            async_session.add(task)

        # User 2 tasks
        for i in range(3):
            task = Task(
                type="summarize_document",
                input={"text": f"user2_task{i}"},
                status="done",
                user_id_hash=user2_hash,
                input_tokens=200,
                output_tokens=100,
                total_cost=0.00002,
            )
            async_session.add(task)

        await async_session.commit()

        # Query user 1 costs
        response1 = await client.get(f"/tasks/costs/by-user/{user1_hash}")
        data1 = response1.json()
        assert data1["total_tasks"] == 2
        assert data1["total_cost"] == pytest.approx(0.00002, abs=0.000001)

        # Query user 2 costs
        response2 = await client.get(f"/tasks/costs/by-user/{user2_hash}")
        data2 = response2.json()
        assert data2["total_tasks"] == 3
        assert data2["total_cost"] == pytest.approx(0.00006, abs=0.000001)

        # Verify summary includes both
        summary_response = await client.get("/tasks/costs/summary")
        summary = summary_response.json()
        assert summary["unique_users"] == 2
        assert summary["total_tasks"] == 5
        assert summary["total_cost"] == pytest.approx(0.00008, abs=0.000001)
import os
import uuid
from unittest.mock import MagicMock

import openai
import pytest
from dotenv import load_dotenv

from app.agents.assessment_agent import AssessmentAgent
from app.agents.research_agent import ResearchAgent
from app.orchestrator.research_assessment import ResearchAssessmentOrchestrator

# Load environment variables from .env file
load_dotenv()

# Skip all tests in this module if no API key is present
pytestmark = pytest.mark.skipif(
    not os.getenv("OPENROUTER_API_KEY"), reason="OPENROUTER_API_KEY not set"
)


class TestE2EWorkflow:
    """End-to-end workflow tests using real LLM calls."""

    def test_real_research_assessment_workflow(self):  # noqa: PLR0915
        """
        Execute a real research-assessment workflow with live LLM calls.

        Verifies:
        1. Research agent produces actual findings
        2. Assessment agent evaluates them
        3. Workflow completes successfully
        4. Real costs are tracked
        """
        # Use a simple topic that's easy to research and assess
        topic = "The history of the Python programming language"
        task_id = str(uuid.uuid4())

        # We still mock the DB connection since we don't want to depend on a real DB
        # But we use real agents and orchestrator logic
        mock_conn = MagicMock()

        # Setup in-memory state storage to replace DB calls
        # This allows the orchestrator to "persist" state during the test
        workflow_state_storage = {}
        subtask_storage = {}

        # Mock DB functions to use our in-memory storage
        def mock_create_state(
            parent_id,
            workflow_type,
            initial_state,
            max_iterations,
            conn,
            state_data=None,
            tenant_id=None,
        ):
            workflow_state_storage[parent_id] = {
                "parent_id": parent_id,
                "workflow_type": workflow_type,
                "current_state": initial_state,
                "max_iterations": max_iterations,
                "current_iteration": 1,
                "state_data": state_data or {},
                "status": "running",
            }

        def mock_update_state(
            parent_id, current_state, conn, state_data=None, current_iteration=None
        ):
            if parent_id in workflow_state_storage:
                state = workflow_state_storage[parent_id]
                state["current_state"] = current_state
                if state_data:
                    state["state_data"] = state_data
                if current_iteration:
                    state["current_iteration"] = current_iteration
                if current_state == "completed":
                    state["status"] = "completed"

        def mock_get_state(parent_id, conn):
            return workflow_state_storage.get(parent_id)

        def mock_create_subtask(
            parent_id, agent_type, iteration, input_data, conn, user_id_hash=None, tenant_id=None
        ):
            subtask_id = str(uuid.uuid4())
            subtask_storage[subtask_id] = {
                "id": subtask_id,
                "parent_task_id": parent_id,
                "agent_type": agent_type,
                "iteration": iteration,
                "input": input_data,
                "status": "pending",
            }
            return subtask_id

        def mock_get_subtask(subtask_id, conn):
            return subtask_storage.get(subtask_id)

        # Apply mocks
        with pytest.MonkeyPatch.context() as m:
            m.setattr(
                "app.orchestrator.research_assessment.create_workflow_state", mock_create_state
            )
            m.setattr(
                "app.orchestrator.research_assessment.update_workflow_state", mock_update_state
            )
            m.setattr("app.orchestrator.research_assessment.get_workflow_state", mock_get_state)
            m.setattr("app.orchestrator.research_assessment.create_subtask", mock_create_subtask)
            m.setattr("app.orchestrator.research_assessment.get_subtask_by_id", mock_get_subtask)

            # Initialize orchestrator
            orchestrator = ResearchAssessmentOrchestrator(max_iterations=2)

            # 1. Start Workflow
            print(f"\nStarting E2E workflow for topic: {topic}")
            orchestrator.create_workflow(
                parent_task_id=task_id, input_data={"topic": topic}, conn=mock_conn
            )

            # Verify initial state
            state = workflow_state_storage[task_id]
            assert state["current_state"] == "research"
            assert len(subtask_storage) == 1

            # Get the research subtask
            research_subtask_id = next(iter(subtask_storage.keys()))
            research_subtask = subtask_storage[research_subtask_id]
            assert research_subtask["agent_type"] == "research"

            # 2. Execute Research Agent (Real Call)
            print("Executing Research Agent (this may take a few seconds)...")

            research_agent = ResearchAgent()

            try:
                research_result = research_agent.execute(research_subtask["input"])
            except openai.AuthenticationError:
                pytest.xfail(
                    "Authentication failed with OpenRouter. Please check your OPENROUTER_API_KEY in .env"
                )
            except Exception as e:
                pytest.fail(f"Research agent execution failed: {e!s}")

            # Verify research output
            assert "output" in research_result
            assert "findings" in research_result["output"]
            assert len(research_result["output"]["findings"]) > 0

            # Verify usage/cost tracking
            assert "usage" in research_result
            assert research_result["usage"]["total_cost"] > 0
            print(f"Research cost: ${research_result['usage']['total_cost']:.6f}")

            # 3. Process Research Completion
            result = orchestrator.process_subtask_completion(
                parent_task_id=task_id,
                subtask_id=research_subtask_id,
                subtask_output=research_result["output"],
                conn=mock_conn,
            )

            assert result["action"] == "continue"

            # Verify transition to assessment
            state = workflow_state_storage[task_id]
            assert state["current_state"] == "assessment"

            # Find the new assessment subtask
            # We expect 2 subtasks now
            assert len(subtask_storage) == 2
            assessment_subtask = None
            for _sid, task in subtask_storage.items():
                if task["agent_type"] == "assessment":
                    assessment_subtask = task
                    break

            assert assessment_subtask is not None

            # 4. Execute Assessment Agent (Real Call)
            print("Executing Assessment Agent...")

            assessment_agent = AssessmentAgent()

            # Add original topic to input as expected by agent
            assessment_input = assessment_subtask["input"]
            assessment_input["original_topic"] = topic

            try:
                assessment_result = assessment_agent.execute(assessment_input)
            except Exception as e:
                pytest.fail(f"Assessment agent execution failed: {e!s}")

            # Verify assessment output
            assert "output" in assessment_result
            assert "approved" in assessment_result["output"]
            assert isinstance(assessment_result["output"]["approved"], bool)

            # Verify usage/cost tracking
            assert "usage" in assessment_result
            assert assessment_result["usage"]["total_cost"] > 0
            print(f"Assessment cost: ${assessment_result['usage']['total_cost']:.6f}")

            # 5. Process Assessment Completion
            result = orchestrator.process_subtask_completion(
                parent_task_id=task_id,
                subtask_id=assessment_subtask["id"],
                subtask_output=assessment_result["output"],
                conn=mock_conn,
            )

            # Depending on approval, it should either complete or continue
            if assessment_result["output"]["approved"]:
                print("Assessment approved! Workflow completed.")
                assert result["action"] == "complete"
                assert result["output"]["status"] == "completed_approved"
            else:
                print("Assessment rejected. Starting refinement iteration.")
                assert result["action"] == "continue"
                # Should have created a new research subtask (iteration 2)
                assert len(subtask_storage) == 3
from app.agents.base import extract_json


def test_extract_json_standard():
    """Test extracting standard JSON."""
    text = '{"key": "value"}'
    result = extract_json(text)
    assert result == {"key": "value"}


def test_extract_json_markdown():
    """Test extracting JSON from markdown."""
    text = 'Here is the result:\n```json\n{"key": "value"}\n```'
    result = extract_json(text)
    assert result == {"key": "value"}


def test_extract_json_sse_prefix():
    """Test extracting JSON with SSE data: prefix."""
    text = 'data: {"key": "value"}'
    result = extract_json(text)
    assert result == {"key": "value"}


def test_extract_json_sse_prefix_with_newlines():
    """Test extracting JSON with SSE data: prefix and newlines."""
    text = 'data: \n{"key": "value"}\n'
    result = extract_json(text)
    assert result == {"key": "value"}


def test_extract_json_embedded_sse():
    """Test extracting JSON where data: is inside text but regex finds JSON."""
    # This mimics if the model outputs "data: " then the JSON block
    text = 'data: {"key": "value"}'
    result = extract_json(text)
    assert result == {"key": "value"}
"""Tests for database models."""

from uuid import UUID

import pytest
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.models import Task


@pytest.mark.asyncio
class TestTaskModel:
    """Tests for Task ORM model."""

    async def test_create_task(self, async_session: AsyncSession):
        """Test creating a task in database."""
        task = Task(type="summarize_document", status="pending", input={"text": "Test document"})

        async_session.add(task)
        await async_session.commit()
        await async_session.refresh(task)

        assert task.id is not None
        assert isinstance(task.id, UUID)
        assert task.type == "summarize_document"
        assert task.status == "pending"
        assert task.input == {"text": "Test document"}
        assert task.output is None
        assert task.error is None
        assert task.created_at is not None
        assert task.updated_at is not None

    async def test_task_with_output(self, async_session: AsyncSession):
        """Test creating task with output."""
        task = Task(
            type="summarize_document",
            status="done",
            input={"text": "Test"},
            output={"summary": "Test summary"},
        )

        async_session.add(task)
        await async_session.commit()
        await async_session.refresh(task)

        assert task.status == "done"
        assert task.output["summary"] == "Test summary"

    async def test_task_with_error(self, async_session: AsyncSession):
        """Test creating task with error."""
        task = Task(
            type="test_task", status="error", input={"data": "test"}, error="Processing failed"
        )

        async_session.add(task)
        await async_session.commit()
        await async_session.refresh(task)

        assert task.status == "error"
        assert task.error == "Processing failed"

    async def test_query_task_by_id(self, async_session: AsyncSession):
        """Test querying task by ID."""
        # Create task
        task = Task(type="test_task", status="pending", input={"data": "test"})
        async_session.add(task)
        await async_session.commit()
        await async_session.refresh(task)

        task_id = task.id

        # Query by ID
        result = await async_session.execute(select(Task).where(Task.id == task_id))
        found_task = result.scalar_one_or_none()

        assert found_task is not None
        assert found_task.id == task_id
        assert found_task.type == "test_task"

    async def test_query_tasks_by_status(self, async_session: AsyncSession):
        """Test querying tasks by status."""
        # Create multiple tasks
        pending_task = Task(type="task1", status="pending", input={})
        done_task = Task(type="task2", status="done", input={})

        async_session.add_all([pending_task, done_task])
        await async_session.commit()

        # Query pending tasks
        result = await async_session.execute(select(Task).where(Task.status == "pending"))
        pending_tasks = result.scalars().all()

        assert len(pending_tasks) >= 1
        assert all(t.status == "pending" for t in pending_tasks)

    async def test_update_task(self, async_session: AsyncSession):
        """Test updating a task."""
        # Create task
        task = Task(type="test_task", status="pending", input={"data": "test"})
        async_session.add(task)
        await async_session.commit()
        await async_session.refresh(task)

        # Update task
        task.status = "done"
        task.output = {"result": "success"}

        await async_session.commit()
        await async_session.refresh(task)

        assert task.status == "done"
        assert task.output["result"] == "success"

    async def test_task_jsonb_fields(self, async_session: AsyncSession):
        """Test JSONB fields store complex data."""
        complex_input = {
            "nested": {"field1": "value1", "field2": [1, 2, 3]},
            "list": ["a", "b", "c"],
        }

        task = Task(type="complex_task", status="pending", input=complex_input)

        async_session.add(task)
        await async_session.commit()
        await async_session.refresh(task)

        assert task.input == complex_input
        assert task.input["nested"]["field1"] == "value1"
        assert task.input["list"] == ["a", "b", "c"]
from unittest.mock import MagicMock, patch

import pytest

from app.orchestrator.research_assessment import ResearchAssessmentOrchestrator


@pytest.fixture
def mock_db_utils():
    with (
        patch("app.orchestrator.research_assessment.get_workflow_state") as mock_get_state,
        patch("app.orchestrator.research_assessment.get_subtask_by_id") as mock_get_subtask,
        patch("app.orchestrator.research_assessment.update_workflow_state") as mock_update_state,
    ):
        yield mock_get_state, mock_get_subtask, mock_update_state


def test_process_assessment_completion_approved(mock_db_utils):
    """Test that approved assessment returns the full final output."""
    mock_get_state, mock_get_subtask, _mock_update_state = mock_db_utils

    orchestrator = ResearchAssessmentOrchestrator()
    conn = MagicMock()

    # Mock workflow state with previous research
    mock_get_state.return_value = {
        "current_state": "assessment",
        "current_iteration": 1,
        "state_data": {"research_iteration_1": {"findings": "Great research"}},
    }

    # Mock subtask
    mock_get_subtask.return_value = {"agent_type": "assessment"}

    # Mock assessment output (approved)
    assessment_output = {"approved": True, "feedback": "Good job"}

    result = orchestrator.process_subtask_completion(
        parent_task_id="parent-123",
        subtask_id="subtask-123",
        subtask_output=assessment_output,
        conn=conn,
    )

    # Verify result contains the full output
    assert result["action"] == "complete"
    assert "output" in result
    assert result["output"]["status"] == "completed_approved"
    assert result["output"]["research_findings"] == {"findings": "Great research"}
    assert result["output"]["final_assessment"] == assessment_output


def test_process_assessment_completion_max_iterations(mock_db_utils):
    """Test that max iterations returns the full final output."""
    mock_get_state, mock_get_subtask, _mock_update_state = mock_db_utils

    orchestrator = ResearchAssessmentOrchestrator(max_iterations=1)
    conn = MagicMock()

    # Mock workflow state (at max iterations)
    mock_get_state.return_value = {
        "current_state": "assessment",
        "current_iteration": 1,
        "state_data": {"research_iteration_1": {"findings": "Okay research"}},
    }

    mock_get_subtask.return_value = {"agent_type": "assessment"}

    # Mock assessment output (not approved)
    assessment_output = {"approved": False, "feedback": "Needs work"}

    result = orchestrator.process_subtask_completion(
        parent_task_id="parent-123",
        subtask_id="subtask-123",
        subtask_output=assessment_output,
        conn=conn,
    )

    # Verify result contains the full output despite failure to approve
    assert result["action"] == "complete"
    assert "output" in result
    assert result["output"]["status"] == "completed_max_iterations"
    assert result["output"]["research_findings"] == {"findings": "Okay research"}
"""Tests for Agent Registry integration with orchestrators and workers."""

import pytest

from app.agents.registry_init import registry
from app.agents import get_agent
from app.agents.research_agent import ResearchAgent
from app.agents.assessment_agent import AssessmentAgent


def test_registry_initialized():
    """Test that registry is initialized on import."""
    # Should have agents either from YAML or auto-discovery
    assert registry is not None

    # Should have at least research and assessment
    assert registry.has("research") or registry.has("assessment")


def test_get_agent_uses_registry():
    """Test that get_agent() uses registry."""
    # Get agent via get_agent function
    research_agent = get_agent("research")

    # Should return an instance
    assert research_agent is not None
    assert isinstance(research_agent, ResearchAgent)


def test_get_agent_returns_singleton():
    """Test that get_agent() returns singleton from registry."""
    # Get same agent twice
    agent1 = get_agent("research")
    agent2 = get_agent("research")

    # Should be same instance (singleton)
    assert agent1 is agent2


def test_get_agent_fallback():
    """Test that get_agent() falls back to hardcoded mapping."""
    # Even if registry fails, should still work
    try:
        agent = get_agent("assessment")
        assert agent is not None
        assert isinstance(agent, AssessmentAgent)
    except ValueError:
        pytest.skip("Expected agent not found")


def test_registry_yaml_config_loaded():
    """Test that YAML config is loaded if present."""
    from pathlib import Path

    yaml_path = Path("config/agents.yaml")
    if not yaml_path.exists():
        pytest.skip("config/agents.yaml not found")

    # Should have loaded from YAML
    assert registry.has("research")
    assert registry.has("assessment")

    # Check metadata
    metadata = registry.get_metadata("research")
    assert metadata.agent_class == ResearchAgent


def test_registry_auto_discovery():
    """Test that auto-discovery works as fallback."""
    # Registry should discover agents from app/agents/
    # even without YAML config
    agent_types = registry.list_all()

    # Should have found at least research and assessment
    assert "research" in agent_types or "assessment" in agent_types


def test_unknown_agent_raises_error():
    """Test that unknown agent type raises clear error."""
    with pytest.raises(ValueError, match="Unknown agent type"):
        get_agent("nonexistent_agent")


def test_worker_can_use_registry():
    """Test that worker's get_agent function works with registry."""
    # Simulate what worker does
    from app.agents import get_agent as worker_get_agent

    agent = worker_get_agent("research")
    assert agent is not None
    assert hasattr(agent, "execute")


def test_registry_config_accessibility():
    """Test that we can access agent config from registry."""
    if not registry.has("research"):
        pytest.skip("Research agent not in registry")

    metadata = registry.get_metadata("research")

    # Config should be accessible
    assert isinstance(metadata.config, dict)
    assert isinstance(metadata.tools, list)
    assert isinstance(metadata.description, str)
"""Tests for Pydantic schemas."""

from datetime import UTC, datetime
from uuid import UUID, uuid4

import pytest
from pydantic import ValidationError

from app.schemas import TaskCreate, TaskResponse, TaskStatus, TaskStatusUpdate, TaskUpdate


@pytest.mark.unit
class TestTaskStatus:
    """Tests for TaskStatus enum."""

    def test_task_status_values(self):
        """Test all task status enum values."""
        assert TaskStatus.PENDING == "pending"
        assert TaskStatus.RUNNING == "running"
        assert TaskStatus.DONE == "done"
        assert TaskStatus.ERROR == "error"

    def test_task_status_from_string(self):
        """Test creating TaskStatus from string."""
        status = TaskStatus("pending")
        assert status == TaskStatus.PENDING


@pytest.mark.unit
class TestTaskCreate:
    """Tests for TaskCreate schema."""

    def test_valid_task_create(self):
        """Test creating valid task."""
        data = {"type": "summarize_document", "input": {"text": "Test document"}}
        task = TaskCreate(**data)
        assert task.type == "summarize_document"
        assert task.input == {"text": "Test document"}

    def test_task_create_missing_type(self):
        """Test creating task without type."""
        with pytest.raises(ValidationError):
            TaskCreate(input={"text": "Test"})

    def test_task_create_missing_input(self):
        """Test creating task without input."""
        with pytest.raises(ValidationError):
            TaskCreate(type="summarize_document")

    def test_task_create_complex_input(self):
        """Test creating task with complex input."""
        data = {
            "type": "analyze_table",
            "input": {
                "table_name": "users",
                "columns": [{"name": "id", "type": "int"}, {"name": "email", "type": "varchar"}],
                "business_context": "User management",
            },
        }
        task = TaskCreate(**data)
        assert task.input["table_name"] == "users"
        assert len(task.input["columns"]) == 2


@pytest.mark.unit
class TestTaskUpdate:
    """Tests for TaskUpdate schema."""

    def test_valid_task_update_status(self):
        """Test updating task status."""
        update = TaskUpdate(status=TaskStatus.DONE)
        assert update.status == TaskStatus.DONE
        assert update.output is None
        assert update.error is None

    def test_valid_task_update_output(self):
        """Test updating task with output."""
        output = {"summary": "Test summary"}
        update = TaskUpdate(output=output)
        assert update.output == output
        assert update.status is None

    def test_valid_task_update_error(self):
        """Test updating task with error."""
        update = TaskUpdate(error="Test error message")
        assert update.error == "Test error message"

    def test_task_update_all_fields(self):
        """Test updating all fields."""
        update = TaskUpdate(status=TaskStatus.ERROR, output=None, error="Processing failed")
        assert update.status == TaskStatus.ERROR
        assert update.error == "Processing failed"


@pytest.mark.unit
class TestTaskResponse:
    """Tests for TaskResponse schema."""

    def test_task_response_from_dict(self):
        """Test creating TaskResponse from dict."""
        data = {
            "id": uuid4(),
            "type": "summarize_document",
            "status": "pending",
            "input": {"text": "Test"},
            "output": None,
            "error": None,
            "created_at": datetime.now(UTC),
            "updated_at": datetime.now(UTC),
        }
        task = TaskResponse(**data)
        assert isinstance(task.id, UUID)
        assert task.type == "summarize_document"
        assert task.status == "pending"

    def test_task_response_with_output(self):
        """Test TaskResponse with output data."""
        data = {
            "id": uuid4(),
            "type": "summarize_document",
            "status": "done",
            "input": {"text": "Test"},
            "output": {"summary": "Test summary"},
            "error": None,
            "created_at": datetime.now(UTC),
            "updated_at": datetime.now(UTC),
        }
        task = TaskResponse(**data)
        assert task.status == "done"
        assert task.output["summary"] == "Test summary"


@pytest.mark.unit
class TestTaskStatusUpdate:
    """Tests for TaskStatusUpdate schema (WebSocket)."""

    def test_task_status_update_creation(self):
        """Test creating TaskStatusUpdate."""
        task_id = uuid4()
        update = TaskStatusUpdate(
            task_id=task_id,
            status="running",
            type="summarize_document",
            output=None,
            error=None,
            updated_at=datetime.now(UTC),
        )
        assert update.task_id == task_id
        assert update.status == "running"
        assert update.type == "summarize_document"

    def test_task_status_update_with_output(self):
        """Test TaskStatusUpdate with output."""
        update = TaskStatusUpdate(
            task_id=uuid4(),
            status="done",
            type="summarize_document",
            output={"summary": "Completed"},
            error=None,
            updated_at=datetime.now(UTC),
        )
        assert update.status == "done"
        assert update.output["summary"] == "Completed"
"""Tests for WebSocket functionality."""

from datetime import UTC, datetime
from uuid import uuid4

import pytest
from fastapi.testclient import TestClient
from httpx import AsyncClient

from app.main import app
from app.schemas import TaskStatusUpdate
from app.websocket import manager


class TestWebSocketManager:
    """Tests for WebSocket connection manager."""

    def test_manager_initialization(self):
        """Test manager initializes with empty connections."""
        assert isinstance(manager.active_connections, set)

    @pytest.mark.asyncio
    async def test_broadcast_with_no_connections(self):
        """Test broadcast with no active connections doesn't error."""
        # Should not raise an error
        update = TaskStatusUpdate(
            task_id=uuid4(),
            status="running",
            type="summarize_document",
            output=None,
            error=None,
            updated_at=datetime.now(UTC),
        )
        await manager.broadcast(update)


@pytest.mark.integration
class TestWebSocketEndpoint:
    """Integration tests for WebSocket endpoint."""

    def test_websocket_connection(self):
        """Test WebSocket connection and ping/pong."""
        client = TestClient(app)

        with client.websocket_connect("/ws") as websocket:
            # Send ping
            websocket.send_text("ping")

            # Receive pong
            response = websocket.receive_text()
            assert response == "pong"

    def test_websocket_receives_task_updates(self):
        """Test WebSocket receives task creation updates."""
        client = TestClient(app)

        # Connect to WebSocket
        with client.websocket_connect("/ws") as websocket:
            # In another "thread" (simulated), create a task
            # Note: This is a simplified test. In reality, you'd need
            # to coordinate the task creation with the WebSocket listener

            # For now, just verify connection works
            websocket.send_text("ping")
            response = websocket.receive_text()
            assert response == "pong"


@pytest.mark.integration
class TestWebSocketTaskBroadcast:
    """Test task updates are broadcast via WebSocket."""

    async def test_task_creation_broadcasts(self, client: AsyncClient, sample_task_data):
        """Test that creating a task triggers WebSocket broadcast."""
        # Note: This test verifies the API calls the broadcast method
        # Full WebSocket integration testing is complex in async context

        # Create task (should trigger broadcast internally)
        response = await client.post("/tasks", json=sample_task_data)
        assert response.status_code == 201

        # Task was created successfully
        # In production, a connected WebSocket client would receive this update
        data = response.json()
        assert data["status"] == "pending"

    async def test_task_update_broadcasts(self, client: AsyncClient, sample_task_data):
        """Test that updating a task triggers WebSocket broadcast."""
        # Create task
        create_response = await client.post("/tasks", json=sample_task_data)
        task_id = create_response.json()["id"]

        # Update task (should trigger broadcast)
        update_response = await client.patch(
            f"/tasks/{task_id}", json={"status": "done", "output": {"result": "success"}}
        )
        assert update_response.status_code == 200

        # Verify update was saved
        data = update_response.json()
        assert data["status"] == "done"
"""Tests for lease-based task acquisition mechanism."""

import os
import socket
from datetime import UTC, datetime, timedelta
from unittest.mock import MagicMock, patch

from app.config import settings
from app.worker_lease import recover_expired_leases, renew_lease


class TestLeaseAcquisition:
    """Test lease-based task acquisition logic."""

    @patch("app.worker_lease.logger")
    def test_lease_recovery_expired_tasks(self, mock_logger):  # noqa: ARG002
        """Test recovery of tasks with expired leases."""
        # Mock database connection
        mock_conn = MagicMock()
        mock_cur = MagicMock()
        mock_conn.cursor.return_value = mock_cur

        # Mock recovered tasks
        mock_cur.fetchall.side_effect = [
            # First call: recovered tasks
            [
                ("task-1", "summarize", 1, "old-worker:1"),
                ("task-2", "research", 2, "old-worker:2"),
            ],
            # Second call: exhausted tasks
            [("task-3", "summarize")],
            # Third call: recovered subtasks
            [("subtask-1", "research", 1)],
            # Fourth call: exhausted subtasks
            [],
        ]

        result = recover_expired_leases(mock_conn, "new-worker:1")

        # Verify count
        assert result == 3  # 2 tasks + 1 subtask recovered

        # Verify SQL was executed to recover tasks
        assert mock_cur.execute.call_count >= 4

        # Verify commit was called
        assert mock_conn.commit.called

    @patch("app.worker_lease.logger")
    def test_lease_recovery_max_retries_exhausted(self, mock_logger):
        """Test tasks that exceed max retries are marked as error."""
        mock_conn = MagicMock()
        mock_cur = MagicMock()
        mock_conn.cursor.return_value = mock_cur

        # Mock no recovered tasks, but some exhausted
        mock_cur.fetchall.side_effect = [
            [],  # Recovered tasks
            [("task-exhausted", "summarize")],  # Exhausted tasks
            [],  # Recovered subtasks
            [],  # Exhausted subtasks
        ]

        result = recover_expired_leases(mock_conn, "worker:1")

        assert result == 0  # No tasks recovered (exhausted ones marked as error)

        # Verify exhausted task was logged
        assert any("task_retry_exhausted" in str(call) for call in mock_logger.error.call_args_list)

    def test_lease_renewal_success(self):
        """Test successful lease renewal."""
        mock_conn = MagicMock()
        mock_cur = MagicMock()
        mock_conn.cursor.return_value = mock_cur
        mock_cur.rowcount = 1  # Simulate successful update

        result = renew_lease(mock_conn, "task-123", "task", "worker:1")

        assert result is True
        assert mock_conn.commit.called
        assert mock_cur.execute.called

    def test_lease_renewal_wrong_owner(self):
        """Test lease renewal fails when worker doesn't own the task."""
        mock_conn = MagicMock()
        mock_cur = MagicMock()
        mock_conn.cursor.return_value = mock_cur
        mock_cur.rowcount = 0  # Simulate no rows updated

        result = renew_lease(mock_conn, "task-123", "task", "wrong-worker:1")

        assert result is False
        # No commit since update failed
        assert not mock_conn.commit.called


class TestAdaptivePolling:
    """Test adaptive polling backoff logic."""

    def test_backoff_increases_when_no_tasks(self):
        """Test polling interval increases when queue is empty."""
        poll_interval = settings.worker_poll_min_interval_seconds  # 0.2

        # Simulate 5 empty polls
        for _ in range(5):
            poll_interval = min(
                poll_interval * settings.worker_poll_backoff_multiplier,
                settings.worker_poll_max_interval_seconds,
            )

        # Should increase exponentially but not exceed max
        assert poll_interval > settings.worker_poll_min_interval_seconds
        assert poll_interval <= settings.worker_poll_max_interval_seconds

    def test_backoff_resets_on_task_found(self):
        """Test polling interval resets when task is found."""
        # Start with backed-off interval
        poll_interval = 5.0

        # Task found - reset to minimum
        poll_interval = settings.worker_poll_min_interval_seconds

        assert poll_interval == settings.worker_poll_min_interval_seconds

    def test_backoff_caps_at_max(self):
        """Test polling interval doesn't exceed maximum."""
        poll_interval = settings.worker_poll_min_interval_seconds

        # Simulate many empty polls
        for _ in range(20):
            poll_interval = min(
                poll_interval * settings.worker_poll_backoff_multiplier,
                settings.worker_poll_max_interval_seconds,
            )

        # Should never exceed max
        assert poll_interval == settings.worker_poll_max_interval_seconds


class TestRetryLogic:
    """Test task retry logic with try_count and max_tries."""

    def test_task_retried_within_max_tries(self):
        """Test task is retried if try_count < max_tries."""
        # Simulate task selection query logic
        task = {
            "id": "task-123",
            "type": "summarize",
            "try_count": 2,
            "max_tries": 3,
            "status": "pending",
        }

        # Task should be eligible for retry
        assert task["try_count"] < task["max_tries"]
        assert task["status"] == "pending"

    def test_task_not_retried_when_max_exceeded(self):
        """Test task is not retried if try_count >= max_tries."""
        task = {
            "id": "task-456",
            "type": "summarize",
            "try_count": 3,
            "max_tries": 3,
            "status": "error",
        }

        # Task should NOT be eligible for retry
        assert task["try_count"] >= task["max_tries"]

    def test_try_count_incremented_on_acquisition(self):
        """Test try_count is incremented when task is acquired."""
        mock_conn = MagicMock()
        mock_cur = MagicMock()
        mock_conn.cursor.return_value = mock_cur

        task_id = "task-789"
        worker_id = "worker:1"
        current_try_count = 1

        # Simulate the UPDATE query that claims the task
        mock_cur.execute(
            """
            UPDATE tasks
            SET status = 'running',
                locked_at = NOW(),
                locked_by = %s,
                lease_timeout = %s,
                try_count = try_count + 1,
                updated_at = NOW()
            WHERE id = %s
            """,
            (worker_id, datetime.now(UTC), task_id),
        )

        # After execution, try_count should be incremented
        # (In real code, this happens in the database)
        expected_try_count = current_try_count + 1
        assert expected_try_count == 2


class TestLeaseTimeout:
    """Test lease timeout calculations and expired lease detection."""

    def test_lease_timeout_calculation(self):
        """Test lease timeout is correctly calculated."""
        now = datetime.now(UTC)
        lease_duration = timedelta(seconds=settings.worker_lease_duration_seconds)
        expected_timeout = now + lease_duration

        # Verify timeout is in the future
        assert expected_timeout > now

        # Verify it's approximately the configured duration
        diff = (expected_timeout - now).total_seconds()
        assert abs(diff - settings.worker_lease_duration_seconds) < 1

    def test_expired_lease_detection(self):
        """Test detection of expired leases."""
        # Simulate expired lease
        expired_timeout = datetime.now(UTC) - timedelta(minutes=1)
        now = datetime.now(UTC)

        assert expired_timeout < now  # Lease is expired

    def test_active_lease_detection(self):
        """Test detection of active (non-expired) leases."""
        # Simulate active lease
        active_timeout = datetime.now(UTC) + timedelta(minutes=4)
        now = datetime.now(UTC)

        assert active_timeout > now  # Lease is still active


class TestWorkerIdentity:
    """Test worker ID generation and uniqueness."""

    @patch("socket.gethostname", return_value="test-host")
    @patch("os.getpid", return_value=1234)
    def test_worker_id_format(self, mock_pid, mock_hostname):  # noqa: ARG002
        """Test worker ID has correct format."""
        worker_id = f"{socket.gethostname()}:{os.getpid()}"

        assert worker_id == "test-host:1234"
        assert ":" in worker_id
        assert len(worker_id.split(":")) == 2

    def test_worker_id_uniqueness(self):
        """Test different workers have different IDs."""
        # Simulate two different workers
        worker1_id = "host1:100"
        worker2_id = "host2:200"

        assert worker1_id != worker2_id


class TestDatabaseQueries:
    """Test SQL query logic for lease-based acquisition."""

    def test_pending_task_query_includes_lease_check(self):
        """Test pending task query checks for expired leases."""
        # This is the actual query structure from worker.py
        query = """
        SELECT id, type, input, try_count, max_tries, 'task' as source_type
        FROM tasks
        WHERE status = 'pending'
          AND try_count < max_tries
          AND (lease_timeout IS NULL OR lease_timeout < NOW())
        ORDER BY created_at ASC
        LIMIT 1
        FOR UPDATE SKIP LOCKED
        """

        # Verify query components
        assert "status = 'pending'" in query
        assert "try_count < max_tries" in query
        assert "lease_timeout IS NULL OR lease_timeout < NOW()" in query
        assert "FOR UPDATE SKIP LOCKED" in query

    def test_lease_acquisition_updates_all_fields(self):
        """Test task acquisition update sets all lease fields."""
        query = """
        UPDATE tasks
        SET status = 'running',
            locked_at = NOW(),
            locked_by = %s,
            lease_timeout = %s,
            try_count = try_count + 1,
            updated_at = NOW()
        WHERE id = %s
        """

        # Verify all required fields are updated
        assert "locked_at" in query
        assert "locked_by" in query
        assert "lease_timeout" in query
        assert "try_count = try_count + 1" in query


# Integration-style test (requires mock DB connection)
class TestLeaseRecoveryIntegration:
    """Integration tests for lease recovery mechanism."""

    def test_recovery_frees_tasks_for_other_workers(self):
        """Test recovered tasks can be claimed by different workers."""
        # Simulate scenario:
        # 1. Worker A claims task
        # 2. Worker A crashes (lease expires)
        # 3. Recovery runs
        # 4. Worker B can claim the task
        # We don't need to assign this to a variable if it's not used
        {
            "id": "task-123",
            "status": "running",
            "locked_by": "worker-a:1",
            "lease_timeout": datetime.now(UTC) - timedelta(minutes=1),  # Expired
            "try_count": 1,
            "max_tries": 3,
        }

        # After recovery, task should be:
        expected_after_recovery = {
            "id": "task-123",
            "status": "pending",  # Reset to pending
            "locked_by": None,  # Cleared
            "lease_timeout": None,  # Cleared
            "try_count": 1,  # Unchanged (incremented on next claim)
            "max_tries": 3,
        }

        # Worker B can now claim it (try_count < max_tries)
        assert expected_after_recovery["try_count"] < expected_after_recovery["max_tries"]
        assert expected_after_recovery["status"] == "pending"
import contextlib
from unittest.mock import MagicMock, patch

import pytest

from app.worker import _process_task_row, notify_api_async, run_worker


@pytest.fixture
def mock_requests():
    with patch("app.worker.requests") as mock:
        yield mock


@pytest.fixture
def mock_db_connection():
    with patch("app.worker.get_connection") as mock:
        conn = MagicMock()
        cur = MagicMock()
        conn.cursor.return_value = cur
        mock.return_value = conn
        yield mock, conn, cur


@pytest.fixture
def mock_execute_task():
    with patch("app.worker.execute_task") as mock:
        yield mock


@pytest.fixture
def mock_tracer():
    with patch("app.worker.tracer") as mock:
        span = MagicMock()
        mock.start_as_current_span.return_value.__enter__.return_value = span
        mock.start_as_current_span.return_value.__enter__.return_value = span
        yield mock, span


@pytest.fixture
def mock_audit_log():
    with patch("app.worker.log_audit_event") as mock:
        yield mock


class TestWorkerNotification:
    def test_notify_api_async_success(self, mock_requests):
        """Test successful API notification."""
        mock_requests.patch.return_value.status_code = 200

        notify_api_async("task-123", "running")

        mock_requests.patch.assert_called_once()
        args, kwargs = mock_requests.patch.call_args
        assert "task-123" in args[0]
        assert kwargs["json"] == {"status": "running"}

    def test_notify_api_async_with_output(self, mock_requests):
        """Test API notification with output."""
        mock_requests.patch.return_value.status_code = 200

        notify_api_async("task-123", "done", output={"result": "ok"})

        _, kwargs = mock_requests.patch.call_args
        assert kwargs["json"] == {"status": "done", "output": {"result": "ok"}}

    def test_notify_api_async_failure(self, mock_requests):
        """Test API notification failure (should not raise exception)."""
        mock_requests.patch.side_effect = Exception("Network error")

        # Should not raise exception
        notify_api_async("task-123", "running")
        mock_requests.patch.assert_called_once()


class TestTaskProcessing:
    @pytest.mark.usefixtures("mock_tracer")
    def test_process_task_success(
        self, mock_db_connection, mock_execute_task, mock_requests, mock_audit_log
    ):
        """Test successful task processing flow."""
        _, conn, cur = mock_db_connection

        # Mock task row
        row = {
            "id": "task-123",
            "type": "test_task",
            "input": {"key": "value", "_trace_context": {}},
        }

        # Mock execution result
        mock_execute_task.return_value = {"output": "result", "usage": {"total_cost": 0.01}}

        _process_task_row(conn, cur, row)

        # Verify DB updates
        # 1. Status update to running
        assert cur.execute.call_count >= 2
        # 2. Status update to done
        call_args_list = cur.execute.call_args_list
        assert "running" in call_args_list[0][0][0]
        assert "done" in call_args_list[1][0][0]

        # Verify API notifications
        assert mock_requests.patch.call_count == 2

        # Verify execute_task called
        mock_execute_task.assert_called_once_with("test_task", {"key": "value"}, None)

        # Verify audit logs (Start + Complete)
        assert mock_audit_log.call_count == 2
        assert mock_audit_log.call_args_list[0][0][1] == "task_started"
        assert mock_audit_log.call_args_list[1][0][1] == "task_completed"

    @pytest.mark.usefixtures("mock_tracer")
    def test_process_task_execution_error(
        self, mock_db_connection, mock_execute_task, mock_requests, mock_audit_log
    ):
        """Test task processing with execution error."""
        _, conn, cur = mock_db_connection

        row = {"id": "task-123", "type": "test_task", "input": {"key": "value"}}

        # Mock execution failure
        mock_execute_task.side_effect = ValueError("Processing failed")

        _process_task_row(conn, cur, row)

        # Verify DB update to error
        call_args_list = cur.execute.call_args_list
        assert "error" in call_args_list[-1][0][0]
        assert "Processing failed" in call_args_list[-1][0][1][0]

        # Verify API notification for error
        assert mock_requests.patch.call_count == 2  # running, then error

        # Verify audit logs (Start + Failed)
        assert mock_audit_log.call_count == 2
        assert mock_audit_log.call_args_list[0][0][1] == "task_started"
        assert mock_audit_log.call_args_list[1][0][1] == "task_failed"

    @pytest.mark.usefixtures("mock_requests", "mock_tracer", "mock_audit_log")
    def test_process_task_legacy_result(self, mock_db_connection, mock_execute_task):
        """Test backward compatibility for legacy result format (direct output)."""
        _, conn, cur = mock_db_connection

        row = {"id": "task-123", "type": "test", "input": {}}
        mock_execute_task.return_value = "direct_output_string"

        _process_task_row(conn, cur, row)

        # Verify done update uses the string output
        call_args = cur.execute.call_args
        # The Json wrapper makes it hard to check exact value, but we can check the structure
        assert "done" in call_args[0][0]


class TestWorkerLoop:
    def test_run_worker_one_loop(self, mock_db_connection):
        """Test one iteration of the worker loop."""
        _, _, cur = mock_db_connection

        # Mock sleep to avoid waiting
        with patch("app.worker.time.sleep"):
            # Mock finding no tasks then raising KeyboardInterrupt to break loop
            # This is a trick to test the loop logic without infinite loop
            # First call: returns None (no task), Second call: raises KeyboardInterrupt to break
            cur.fetchone.side_effect = [None, KeyboardInterrupt("Break Loop")]

            with contextlib.suppress(KeyboardInterrupt):
                run_worker()

        # Verify it tried to fetch a task
        assert cur.execute.called
        assert "SELECT id" in cur.execute.call_args[0][0]
"""Tests for declarative workflow definitions."""

import tempfile
from pathlib import Path

import pytest

from app.workflow_definition import WorkflowDefinition, WorkflowStep


class TestWorkflowStep:
    """Tests for WorkflowStep dataclass."""

    def test_valid_step(self):
        """Test creating valid workflow step."""
        step = WorkflowStep(agent_type="research", name="conduct_research")
        assert step.agent_type == "research"
        assert step.name == "conduct_research"

    def test_missing_agent_type(self):
        """Test that empty agent_type raises error."""
        with pytest.raises(ValueError, match="agent_type is required"):
            WorkflowStep(agent_type="", name="test")


class TestWorkflowDefinition:
    """Tests for WorkflowDefinition dataclass."""

    def test_valid_definition(self):
        """Test creating valid workflow definition."""
        definition = WorkflowDefinition(
            name="test_workflow",
            description="Test workflow",
            steps=[WorkflowStep(agent_type="research")],
            coordination_type="sequential",
        )
        assert definition.name == "test_workflow"
        assert len(definition.steps) == 1
        assert definition.coordination_type == "sequential"

    def test_missing_name(self):
        """Test that missing name raises error."""
        with pytest.raises(ValueError, match="name is required"):
            WorkflowDefinition(
                name="",
                description="Test",
                steps=[WorkflowStep(agent_type="research")],
                coordination_type="sequential",
            )

    def test_missing_steps(self):
        """Test that empty steps raises error."""
        with pytest.raises(ValueError, match="at least one step"):
            WorkflowDefinition(
                name="test",
                description="Test",
                steps=[],
                coordination_type="sequential",
            )

    def test_iterative_without_convergence_check(self):
        """Test that iterative_refinement requires convergence_check."""
        with pytest.raises(ValueError, match="requires a convergence_check"):
            WorkflowDefinition(
                name="test",
                description="Test",
                steps=[WorkflowStep(agent_type="research")],
                coordination_type="iterative_refinement",
            )

    def test_invalid_max_iterations(self):
        """Test that max_iterations must be at least 1."""
        with pytest.raises(ValueError, match="at least 1"):
            WorkflowDefinition(
                name="test",
                description="Test",
                steps=[WorkflowStep(agent_type="research")],
                coordination_type="sequential",
                max_iterations=0,
            )


class TestYAMLParsing:
    """Tests for YAML parsing functionality."""

    def test_load_valid_yaml(self):
        """Test loading valid YAML workflow."""
        yaml_content = """
name: test_workflow
description: Test workflow
coordination_type: sequential
max_iterations: 2

steps:
  - agent_type: research
    name: do_research
  - agent_type: assessment
    name: assess_quality
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
            f.write(yaml_content)
            temp_path = f.name

        try:
            definition = WorkflowDefinition.from_yaml(temp_path)
            assert definition.name == "test_workflow"
            assert definition.description == "Test workflow"
            assert definition.coordination_type == "sequential"
            assert definition.max_iterations == 2
            assert len(definition.steps) == 2
            assert definition.steps[0].agent_type == "research"
            assert definition.steps[0].name == "do_research"
        finally:
            Path(temp_path).unlink()

    def test_load_iterative_yaml(self):
        """Test loading iterative refinement workflow."""
        yaml_content = """
name: iterative_workflow
description: Iterative test
coordination_type: iterative_refinement
max_iterations: 3
convergence_check: assessment_approved

steps:
  - agent_type: research
  - agent_type: assessment
"""
        with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
            f.write(yaml_content)
            temp_path = f.name

        try:
            definition = WorkflowDefinition.from_yaml(temp_path)
            assert definition.coordination_type == "iterative_refinement"
            assert definition.convergence_check == "assessment_approved"
            assert definition.max_iterations == 3
        finally:
            Path(temp_path).unlink()

    def test_load_nonexistent_file(self):
        """Test loading non-existent YAML file."""
        with pytest.raises(FileNotFoundError):
            WorkflowDefinition.from_yaml("/nonexistent/file.yaml")

    def test_load_invalid_yaml(self):
        """Test loading invalid YAML format."""
        yaml_content = "just a string, not a dict"

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
            f.write(yaml_content)
            temp_path = f.name

        try:
            with pytest.raises(ValueError, match="Invalid YAML format"):
                WorkflowDefinition.from_yaml(temp_path)
        finally:
            Path(temp_path).unlink()

    def test_to_dict(self):
        """Test converting definition to dictionary."""
        definition = WorkflowDefinition(
            name="test",
            description="Test workflow",
            steps=[
                WorkflowStep(agent_type="research", name="r1"),
                WorkflowStep(agent_type="assessment"),
            ],
            coordination_type="sequential",
            max_iterations=5,
        )

        result = definition.to_dict()
        assert result["name"] == "test"
        assert result["description"] == "Test workflow"
        assert len(result["steps"]) == 2
        assert result["steps"][0]["agent_type"] == "research"
        assert result["steps"][0]["name"] == "r1"
        assert result["coordination_type"] == "sequential"
        assert result["max_iterations"] == 5
"""Comprehensive integration tests for workflow execution.

These tests validate end-to-end workflow behavior including:
- Sequential workflow execution
- Iterative workflow with convergence
- Max iterations handling
- Failure propagation
- Cost tracking across workflows
- Audit logging across workflows
"""

import tempfile
import uuid
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest

from app.models import AuditLog
from app.orchestrator.research_assessment import ResearchAssessmentOrchestrator
from app.tasks import calculate_cost
from app.workflow_definition import WorkflowDefinition, WorkflowStep

pytestmark = pytest.mark.integration


class TestSequentialWorkflowExecution:
    """Integration tests for sequential workflow execution."""

    @patch("app.orchestrator.research_assessment.create_subtask")
    @patch("app.orchestrator.research_assessment.create_workflow_state")
    @patch("app.orchestrator.research_assessment.get_workflow_state")
    @patch("app.orchestrator.research_assessment.update_workflow_state")
    def test_sequential_workflow_creates_subtasks(
        self,
        mock_update_state,  # noqa: ARG002
        mock_get_state,  # noqa: ARG002
        mock_create_state,
        mock_create_subtask,
    ):
        """Test that sequential workflow creates research then assessment subtasks."""

        task_id = str(uuid.uuid4())
        mock_conn = MagicMock()

        orchestrator = ResearchAssessmentOrchestrator(max_iterations=3)

        # Initialize workflow
        orchestrator.create_workflow(
            parent_task_id=task_id,
            input_data={"topic": "AI Safety"},
            conn=mock_conn,
            user_id_hash="test_user",
        )

        # Verify workflow state created
        mock_create_state.assert_called_once()
        call_args = mock_create_state.call_args[1]
        assert call_args["parent_id"] == task_id
        assert call_args["workflow_type"] == "research_assessment"
        assert call_args["initial_state"] == "research"
        assert call_args["max_iterations"] == 3

        # Verify first research subtask created
        create_subtask_calls = [call[1] for call in mock_create_subtask.call_args_list]
        first_subtask = create_subtask_calls[0]
        assert first_subtask["parent_id"] == task_id
        assert first_subtask["agent_type"] == "research"
        assert first_subtask["iteration"] == 1
        assert "topic" in first_subtask["input_data"]

    @patch("app.orchestrator.research_assessment.create_subtask")
    @patch("app.orchestrator.research_assessment.get_workflow_state")
    @patch("app.orchestrator.research_assessment.update_workflow_state")
    @patch("app.orchestrator.research_assessment.get_subtask_by_id")
    def test_research_completion_triggers_assessment(
        self,
        mock_get_subtask,
        mock_update_state,  # noqa: ARG002
        mock_get_state,
        mock_create_subtask,
    ):
        """Test that research completion triggers assessment subtask creation."""

        task_id = str(uuid.uuid4())
        subtask_id = str(uuid.uuid4())
        mock_conn = MagicMock()

        # Mock workflow state
        mock_get_state.return_value = {
            "current_state": "research",
            "current_iteration": 1,
            "state_data": {},
        }

        # Mock subtask
        mock_get_subtask.return_value = {"agent_type": "research"}

        orchestrator = ResearchAssessmentOrchestrator(max_iterations=3)

        # Process research completion
        research_output = {
            "findings": "Important research findings",
            "sources": ["source1.com"],
            "key_insights": ["Insight 1"],
        }

        result = orchestrator.process_subtask_completion(
            parent_task_id=task_id,
            subtask_id=subtask_id,
            subtask_output=research_output,
            conn=mock_conn,
            user_id_hash="test_user",
        )

        # Should continue to assessment
        assert result["action"] == "continue"

        # Verify assessment subtask created
        mock_create_subtask.assert_called()
        assessment_call = mock_create_subtask.call_args[1]
        assert assessment_call["agent_type"] == "assessment"
        assert assessment_call["iteration"] == 1
        assert "research_findings" in assessment_call["input_data"]

    @patch("app.orchestrator.research_assessment.get_workflow_state")
    @patch("app.orchestrator.research_assessment.get_subtask_by_id")
    def test_assessment_approval_completes_workflow(self, mock_get_subtask, mock_get_state):
        """Test that approved assessment completes the workflow."""

        task_id = str(uuid.uuid4())
        subtask_id = str(uuid.uuid4())
        mock_conn = MagicMock()

        # Mock workflow state with previous research
        mock_get_state.return_value = {
            "current_state": "assessment",
            "current_iteration": 1,
            "state_data": {"research_iteration_1": {"findings": "Great research"}},
        }

        mock_get_subtask.return_value = {"agent_type": "assessment"}

        orchestrator = ResearchAssessmentOrchestrator(max_iterations=3)

        # Process approved assessment
        assessment_output = {
            "approved": True,
            "quality_score": 90,
            "feedback": "Excellent work",
        }

        result = orchestrator.process_subtask_completion(
            parent_task_id=task_id,
            subtask_id=subtask_id,
            subtask_output=assessment_output,
            conn=mock_conn,
        )

        # Workflow should complete
        assert result["action"] == "complete"
        assert "output" in result
        assert result["output"]["status"] == "completed_approved"
        assert "research_findings" in result["output"]
        assert "final_assessment" in result["output"]


class TestIterativeWorkflowRefinement:
    """Integration tests for iterative refinement workflows."""

    @patch("app.orchestrator.research_assessment.create_subtask")
    @patch("app.orchestrator.research_assessment.get_workflow_state")
    @patch("app.orchestrator.research_assessment.update_workflow_state")
    @patch("app.orchestrator.research_assessment.get_subtask_by_id")
    @patch("app.orchestrator.research_assessment.get_task_by_id")
    def test_rejected_assessment_starts_new_iteration(
        self,
        mock_get_task,
        mock_get_subtask,
        mock_update_state,  # noqa: ARG002
        mock_get_state,
        mock_create_subtask,
    ):
        """Test that rejected assessment starts a new iteration."""

        task_id = str(uuid.uuid4())
        subtask_id = str(uuid.uuid4())
        mock_conn = MagicMock()

        # Mock workflow state
        mock_get_state.return_value = {
            "current_state": "assessment",
            "current_iteration": 1,
            "state_data": {"research_iteration_1": {"findings": "Initial findings"}},
        }

        mock_get_subtask.return_value = {"agent_type": "assessment"}
        mock_get_task.return_value = {"input": {"topic": "AI Safety"}}

        orchestrator = ResearchAssessmentOrchestrator(max_iterations=3)

        # Process rejected assessment
        assessment_output = {
            "approved": False,
            "quality_score": 60,
            "feedback": "Needs more detail and sources",
            "suggestions": ["Add citations", "Expand on key points"],
        }

        result = orchestrator.process_subtask_completion(
            parent_task_id=task_id,
            subtask_id=subtask_id,
            subtask_output=assessment_output,
            conn=mock_conn,
        )

        # Should continue with new iteration
        assert result["action"] == "continue"

        # Verify new research subtask created for iteration 2
        mock_create_subtask.assert_called()
        research_call = mock_create_subtask.call_args[1]
        assert research_call["agent_type"] == "research"
        assert research_call["iteration"] == 2
        # Should include feedback from assessment as 'previous_feedback'
        assert "previous_feedback" in research_call["input_data"]
        assert research_call["input_data"]["previous_feedback"] == "Needs more detail and sources"

    @patch("app.orchestrator.research_assessment.get_workflow_state")
    @patch("app.orchestrator.research_assessment.get_subtask_by_id")
    def test_state_data_preserved_across_iterations(self, mock_get_subtask, mock_get_state):
        """Test that state data contains all iteration results."""

        task_id = str(uuid.uuid4())
        subtask_id = str(uuid.uuid4())
        mock_conn = MagicMock()

        # Mock workflow state with multiple iterations
        state_data = {
            "research_iteration_1": {"findings": "First attempt"},
            "research_iteration_2": {"findings": "Revised attempt"},
        }

        mock_get_state.return_value = {
            "current_state": "assessment",
            "current_iteration": 2,
            "state_data": state_data,
        }

        mock_get_subtask.return_value = {"agent_type": "assessment"}

        orchestrator = ResearchAssessmentOrchestrator(max_iterations=3)

        # Process approved assessment from iteration 2
        assessment_output = {"approved": True, "feedback": "Much better!"}

        result = orchestrator.process_subtask_completion(
            parent_task_id=task_id,
            subtask_id=subtask_id,
            subtask_output=assessment_output,
            conn=mock_conn,
        )

        # Verify final output includes latest research findings
        assert result["action"] == "complete"
        assert result["output"]["research_findings"]["findings"] == "Revised attempt"


class TestWorkflowMaxIterations:
    """Integration tests for max iteration boundary conditions."""

    @patch("app.orchestrator.research_assessment.get_workflow_state")
    @patch("app.orchestrator.research_assessment.get_subtask_by_id")
    def test_workflow_completes_at_max_iterations(self, mock_get_subtask, mock_get_state):
        """Test workflow completes when max iterations reached even if not approved."""

        task_id = str(uuid.uuid4())
        subtask_id = str(uuid.uuid4())
        mock_conn = MagicMock()

        # Mock workflow state at max iteration
        mock_get_state.return_value = {
            "current_state": "assessment",
            "current_iteration": 3,
            "state_data": {
                "research_iteration_1": {"findings": "Attempt 1"},
                "research_iteration_2": {"findings": "Attempt 2"},
                "research_iteration_3": {"findings": "Final attempt"},
            },
        }

        mock_get_subtask.return_value = {"agent_type": "assessment"}

        orchestrator = ResearchAssessmentOrchestrator(max_iterations=3)

        # Process rejected assessment at max iteration
        assessment_output = {
            "approved": False,
            "feedback": "Still not perfect",
        }

        result = orchestrator.process_subtask_completion(
            parent_task_id=task_id,
            subtask_id=subtask_id,
            subtask_output=assessment_output,
            conn=mock_conn,
        )

        # Should complete even though not approved
        assert result["action"] == "complete"
        assert "output" in result
        assert result["output"]["status"] == "completed_max_iterations"
        assert "research_findings" in result["output"]
        assert result["output"]["research_findings"]["findings"] == "Final attempt"

    def test_max_iterations_respected_in_orchestrator(self):
        """Test that max_iterations parameter is respected."""

        # Test different max_iterations values
        orch_1 = ResearchAssessmentOrchestrator(max_iterations=1)
        assert orch_1.get_max_iterations() == 1

        orch_3 = ResearchAssessmentOrchestrator(max_iterations=3)
        assert orch_3.get_max_iterations() == 3

        orch_5 = ResearchAssessmentOrchestrator(max_iterations=5)
        assert orch_5.get_max_iterations() == 5


class TestWorkflowCostTracking:
    """Integration tests for cost tracking across workflows."""

    @patch("app.orchestrator.research_assessment.create_subtask")
    @patch("app.orchestrator.research_assessment.get_workflow_state")
    @patch("app.orchestrator.research_assessment.update_workflow_state")
    @patch("app.orchestrator.research_assessment.get_subtask_by_id")
    @patch("app.db_utils.aggregate_subtask_costs")
    def test_cost_aggregation_across_workflow(
        self,
        mock_aggregate_costs,  # noqa: ARG002
        mock_get_subtask,
        mock_update_state,  # noqa: ARG002
        mock_get_state,
        mock_create_subtask,
    ):
        """Test that costs from multiple subtasks aggregate to parent task."""

        task_id = str(uuid.uuid4())
        research_subtask_id = str(uuid.uuid4())
        assessment_subtask_id = str(uuid.uuid4())
        mock_conn = MagicMock()

        orchestrator = ResearchAssessmentOrchestrator(max_iterations=3)

        # Simulate research subtask completion
        mock_get_state.return_value = {
            "current_state": "research",
            "current_iteration": 1,
            "state_data": {},
        }
        mock_get_subtask.return_value = {"agent_type": "research"}

        research_output = {
            "findings": "Research results",
            "sources": ["source1.com"],
        }

        # Process research completion
        orchestrator.process_subtask_completion(
            parent_task_id=task_id,
            subtask_id=research_subtask_id,
            subtask_output=research_output,
            conn=mock_conn,
        )

        # Verify assessment subtask was created
        assert mock_create_subtask.called

        # Now simulate assessment completion
        mock_get_state.return_value = {
            "current_state": "assessment",
            "current_iteration": 1,
            "state_data": {"research_iteration_1": research_output},
        }
        mock_get_subtask.return_value = {"agent_type": "assessment"}

        assessment_output = {"approved": True, "quality_score": 95}

        # Process assessment completion
        orchestrator.process_subtask_completion(
            parent_task_id=task_id,
            subtask_id=assessment_subtask_id,
            subtask_output=assessment_output,
            conn=mock_conn,
        )

        # Note: aggregate_subtask_costs would be called by the worker
        # after each subtask completes. We're verifying the workflow
        # logic is correct, not the worker integration

    def test_cost_calculation_utility(self):
        """Test cost calculation for different models and token counts."""

        # Test Gemini Flash pricing
        cost = calculate_cost("google/gemini-2.5-flash", 1000, 500)
        # Should be: (1000 * 0.075 / 1M) + (500 * 0.30 / 1M) = 0.000225
        assert abs(cost - 0.000225) < 0.000001

        # Test with larger counts
        cost_large = calculate_cost("google/gemini-2.5-flash", 100000, 50000)
        assert abs(cost_large - 0.0225) < 0.0001

        # Test zero tokens
        cost_zero = calculate_cost("google/gemini-2.5-flash", 0, 0)
        assert cost_zero == 0.0

    @patch("app.orchestrator.research_assessment.create_subtask")
    @patch("app.orchestrator.research_assessment.get_workflow_state")
    @patch("app.orchestrator.research_assessment.update_workflow_state")
    @patch("app.orchestrator.research_assessment.get_subtask_by_id")
    @patch("app.orchestrator.research_assessment.get_task_by_id")
    def test_cost_accumulation_across_iterations(
        self,
        mock_get_task,
        mock_get_subtask,
        mock_update_state,  # noqa: ARG002
        mock_get_state,
        mock_create_subtask,
    ):
        """Test that costs accumulate correctly across multiple workflow iterations."""

        task_id = str(uuid.uuid4())
        mock_conn = MagicMock()
        orchestrator = ResearchAssessmentOrchestrator(max_iterations=3)

        # Iteration 1: Research
        mock_get_state.return_value = {
            "current_state": "research",
            "current_iteration": 1,
            "state_data": {},
        }
        mock_get_subtask.return_value = {"agent_type": "research"}

        orchestrator.process_subtask_completion(
            parent_task_id=task_id,
            subtask_id=str(uuid.uuid4()),
            subtask_output={"findings": "iteration 1"},
            conn=mock_conn,
        )

        # Iteration 1: Assessment (rejected)
        mock_get_state.return_value = {
            "current_state": "assessment",
            "current_iteration": 1,
            "state_data": {"research_iteration_1": {"findings": "iteration 1"}},
        }
        mock_get_subtask.return_value = {"agent_type": "assessment"}
        mock_get_task.return_value = {"input": {"topic": "test"}}

        orchestrator.process_subtask_completion(
            parent_task_id=task_id,
            subtask_id=str(uuid.uuid4()),
            subtask_output={"approved": False, "feedback": "needs work"},
            conn=mock_conn,
        )

        # Verify new research subtask created for iteration 2
        create_calls = mock_create_subtask.call_args_list
        # Should have created assessment (iter 1) and research (iter 2)
        assert len(create_calls) >= 2

        # Verify iteration 2 subtask has iteration=2
        iter_2_call = create_calls[-1][1]  # Last call should be iteration 2
        assert iter_2_call["iteration"] == 2
        assert iter_2_call["agent_type"] == "research"

        # In a real workflow, each subtask completion would trigger
        # aggregate_subtask_costs, accumulating costs from all completed
        # subtasks (research iter 1, assessment iter 1, research iter 2, etc.)


class TestWorkflowFailureHandling:
    """Integration tests for workflow failure scenarios."""

    @patch("app.db_utils.get_workflow_state")
    @patch("app.db_utils.get_subtask_by_id")
    def test_invalid_state_transition_returns_failed(self, mock_get_subtask, mock_get_state):
        """Test that invalid state transitions return failed action."""

        task_id = str(uuid.uuid4())
        subtask_id = str(uuid.uuid4())
        mock_conn = MagicMock()

        # Mock state that doesn't match subtask type
        mock_get_state.return_value = {
            "current_state": "assessment",  # Expecting assessment
            "current_iteration": 1,
            "state_data": {},
        }

        # But subtask is research (mismatch)
        mock_get_subtask.return_value = {"agent_type": "research"}

        orchestrator = ResearchAssessmentOrchestrator()

        result = orchestrator.process_subtask_completion(
            parent_task_id=task_id,
            subtask_id=subtask_id,
            subtask_output={"findings": "test"},
            conn=mock_conn,
        )

        # Should fail due to state mismatch
        assert result["action"] == "failed"

    def test_workflow_type_validation(self):
        """Test workflow type validation."""

        # Valid workflow
        orch = ResearchAssessmentOrchestrator()
        assert orch.workflow_type == "research_assessment"

        # Test different max_iterations values
        orch_1 = ResearchAssessmentOrchestrator(max_iterations=1)
        assert orch_1.get_max_iterations() == 1


class TestWorkflowAuditLogging:
    """Integration tests for audit logging across workflows."""

    @patch("app.orchestrator.research_assessment.create_workflow_state")
    @patch("app.orchestrator.research_assessment.create_subtask")
    @patch("app.audit.log_audit_event")
    def test_workflow_initialization_creates_audit_event(
        self,
        mock_audit,  # noqa: ARG002
        mock_create_subtask,
        mock_create_state,
    ):
        """Test that workflow initialization creates audit log entry."""

        task_id = str(uuid.uuid4())
        mock_conn = MagicMock()

        orchestrator = ResearchAssessmentOrchestrator()

        orchestrator.create_workflow(
            parent_task_id=task_id,
            input_data={"topic": "test"},
            conn=mock_conn,
            user_id_hash="user_123",
            tenant_id="tenant_abc",
        )

        # Verify workflow state and subtask creation
        mock_create_state.assert_called_once()
        mock_create_subtask.assert_called_once()

    def test_audit_log_model_structure(self):
        """Test audit log data model structure."""

        # Create a dummy audit log via __init__ (not via ORM)
        log = AuditLog()
        log.event_type = "workflow_initialized"
        log.resource_id = "task-123"
        log.user_id_hash = "user-abc"
        log.metadata_ = {"workflow_type": "research_assessment"}

        # Verify structure
        assert log.event_type == "workflow_initialized"
        assert log.resource_id == "task-123"
        assert log.user_id_hash == "user-abc"
        assert log.metadata_["workflow_type"] == "research_assessment"


class TestDeclarativeWorkflows:
    """Integration tests for declarative workflow definitions."""

    def test_workflow_definition_from_yaml(self):
        """Test loading workflow definition from YAML."""

        yaml_content = """
name: test_sequential
description: Test sequential workflow
coordination_type: sequential
max_iterations: 2

steps:
  - agent_type: research
    name: gather_data
  - agent_type: assessment
    name: validate_data
"""

        with tempfile.NamedTemporaryFile(mode="w", suffix=".yaml", delete=False) as f:
            f.write(yaml_content)
            temp_path = f.name

        try:
            definition = WorkflowDefinition.from_yaml(temp_path)
            assert definition.name == "test_sequential"
            assert definition.coordination_type == "sequential"
            assert len(definition.steps) == 2
            assert definition.steps[0].agent_type == "research"
            assert definition.steps[1].agent_type == "assessment"
        finally:
            Path(temp_path).unlink()

    def test_iterative_workflow_definition(self):
        """Test defining an iterative refinement workflow."""

        definition = WorkflowDefinition(
            name="iterative_test",
            description="Iterative workflow test",
            steps=[
                WorkflowStep(agent_type="research"),
                WorkflowStep(agent_type="assessment"),
            ],
            coordination_type="iterative_refinement",
            max_iterations=3,
            convergence_check="assessment_approved",
        )

        assert definition.coordination_type == "iterative_refinement"
        assert definition.max_iterations == 3
        assert definition.convergence_check == "assessment_approved"

        # Verify validation
        with pytest.raises(ValueError, match="requires a convergence_check"):
            WorkflowDefinition(
                name="invalid",
                description="Missing convergence check",
                steps=[WorkflowStep(agent_type="research")],
                coordination_type="iterative_refinement",
                # Missing convergence_check
            )
"""Tests for workflow registry."""

import tempfile
from pathlib import Path

import pytest

from app.workflow_definition import WorkflowDefinition, WorkflowStep
from app.workflow_registry import WorkflowRegistry


class TestWorkflowRegistry:
    """Tests for WorkflowRegistry."""

    def test_register_workflow(self):
        """Test registering a workflow."""
        registry = WorkflowRegistry()
        definition = WorkflowDefinition(
            name="test",
            description="Test",
            steps=[WorkflowStep(agent_type="research")],
            coordination_type="sequential",
        )

        registry.register(definition)
        assert registry.has("test")
        assert registry.get("test") == definition

    def test_register_duplicate(self):
        """Test that registering duplicate raises error."""
        registry = WorkflowRegistry()
        definition = WorkflowDefinition(
            name="test",
            description="Test",
            steps=[WorkflowStep(agent_type="research")],
            coordination_type="sequential",
        )

        registry.register(definition)
        with pytest.raises(ValueError, match="already registered"):
            registry.register(definition)

    def test_get_nonexistent(self):
        """Test getting non-existent workflow raises error."""
        registry = WorkflowRegistry()
        with pytest.raises(KeyError, match="not found"):
            registry.get("nonexistent")

    def test_list_all(self):
        """Test listing all workflows."""
        registry = WorkflowRegistry()
        definition1 = WorkflowDefinition(
            name="workflow1",
            description="Test 1",
            steps=[WorkflowStep(agent_type="research")],
            coordination_type="sequential",
        )
        definition2 = WorkflowDefinition(
            name="workflow2",
            description="Test 2",
            steps=[WorkflowStep(agent_type="assessment")],
            coordination_type="sequential",
        )

        registry.register(definition1)
        registry.register(definition2)

        workflows = registry.list_all()
        assert len(workflows) == 2
        assert "workflow1" in workflows
        assert "workflow2" in workflows

    def test_load_from_directory(self):
        """Test loading workflows from directory."""
        with tempfile.TemporaryDirectory() as tmpdir:
            tmpdir_path = Path(tmpdir)

            # Create YAML files
            yaml1 = tmpdir_path / "workflow1.yaml"
            yaml1.write_text(
                """
name: workflow1
description: Test 1
coordination_type: sequential

steps:
  - agent_type: research
"""
            )

            yaml2 = tmpdir_path / "workflow2.yml"
            yaml2.write_text(
                """
name: workflow2
description: Test 2
coordination_type: sequential

steps:
  - agent_type: assessment
"""
            )

            # Load from directory
            registry = WorkflowRegistry()
            count = registry.load_from_directory(tmpdir_path)

            assert count == 2
            assert registry.has("workflow1")
            assert registry.has("workflow2")

    def test_load_from_nonexistent_directory(self):
        """Test loading from non-existent directory."""
        registry = WorkflowRegistry()
        with pytest.raises(FileNotFoundError):
            registry.load_from_directory("/nonexistent/directory")

    def test_load_with_invalid_file(self):
        """Test that loading continues even with invalid files."""
        with tempfile.TemporaryDirectory() as tmpdir:
            tmpdir_path = Path(tmpdir)

            # Create valid YAML
            valid_yaml = tmpdir_path / "valid.yaml"
            valid_yaml.write_text(
                """
name: valid
description: Valid workflow
coordination_type: sequential

steps:
  - agent_type: research
"""
            )

            # Create invalid YAML
            invalid_yaml = tmpdir_path / "invalid.yaml"
            invalid_yaml.write_text("not: valid: yaml:")

            # Load from directory - should load valid and skip invalid
            registry = WorkflowRegistry()
            count = registry.load_from_directory(tmpdir_path)

            # Should load only the valid one
            assert count == 1
            assert registry.has("valid")
"""Tests for Agent Registry YAML loading functionality."""

import pytest
from pathlib import Path

from app.agents.registry import AgentRegistry
from app.agents.research_agent import ResearchAgent
from app.agents.assessment_agent import AssessmentAgent


def test_load_valid_yaml(tmp_path):
    """Test loading valid YAML configuration."""
    yaml_content = """
agents:
  - name: test_research
    class: app.agents.research_agent.ResearchAgent
    tools:
      - web_search
    description: "Test research agent"
"""
    yaml_file = tmp_path / "agents.yaml"
    yaml_file.write_text(yaml_content)

    registry = AgentRegistry()
    registry.load_from_yaml(yaml_file)

    assert registry.has("test_research")
    metadata = registry.get_metadata("test_research")
    assert "web_search" in metadata.tools
    assert metadata.description == "Test research agent"


def test_load_yaml_with_config(tmp_path):
    """Test loading YAML with agent configuration."""
    yaml_content = """
agents:
  - name: configured_agent
    class: app.agents.research_agent.ResearchAgent
    config:
      model: gpt-4-turbo
      temperature: 0.7
    tools:
      - web_search
      - document_reader
"""
    yaml_file = tmp_path / "agents.yaml"
    yaml_file.write_text(yaml_content)

    registry = AgentRegistry()
    registry.load_from_yaml(yaml_file)

    metadata = registry.get_metadata("configured_agent")
    assert metadata.config == {"model": "gpt-4-turbo", "temperature": 0.7}
    assert len(metadata.tools) == 2


def test_load_yaml_multiple_agents(tmp_path):
    """Test loading multiple agents from YAML."""
    yaml_content = """
agents:
  - name: research
    class: app.agents.research_agent.ResearchAgent
  - name: assessment
    class: app.agents.assessment_agent.AssessmentAgent
"""
    yaml_file = tmp_path / "agents.yaml"
    yaml_file.write_text(yaml_content)

    registry = AgentRegistry()
    registry.load_from_yaml(yaml_file)

    assert registry.has("research")
    assert registry.has("assessment")
    assert len(registry.list_all()) == 2


def test_load_yaml_missing_file():
    """Test error handling for missing YAML file."""
    registry = AgentRegistry()

    with pytest.raises(FileNotFoundError, match="YAML file not found"):
        registry.load_from_yaml("nonexistent.yaml")


def test_load_yaml_invalid_syntax(tmp_path):
    """Test error handling for invalid YAML syntax."""
    yaml_file = tmp_path / "bad.yaml"
    yaml_file.write_text("not: valid: yaml: [unclosed")

    registry = AgentRegistry()

    with pytest.raises(ValueError, match="Invalid YAML syntax"):
        registry.load_from_yaml(yaml_file)


def test_load_yaml_missing_agents_key(tmp_path):
    """Test error for YAML missing 'agents' key."""
    yaml_file = tmp_path / "no_agents.yaml"
    yaml_file.write_text("other_key: value")

    registry = AgentRegistry()

    with pytest.raises(ValueError, match="must contain 'agents' key"):
        registry.load_from_yaml(yaml_file)


def test_load_yaml_missing_name_field(tmp_path):
    """Test error for agent missing 'name' field."""
    yaml_content = """
agents:
  - class: app.agents.research_agent.ResearchAgent
"""
    yaml_file = tmp_path / "missing_name.yaml"
    yaml_file.write_text(yaml_content)

    registry = AgentRegistry()

    with pytest.raises(ValueError, match="missing required 'name' field"):
        registry.load_from_yaml(yaml_file)


def test_load_yaml_missing_class_field(tmp_path):
    """Test error for agent missing 'class' field."""
    yaml_content = """
agents:
  - name: no_class_agent
"""
    yaml_file = tmp_path / "missing_class.yaml"
    yaml_file.write_text(yaml_content)

    registry = AgentRegistry()

    with pytest.raises(ValueError, match="missing required 'class' field"):
        registry.load_from_yaml(yaml_file)


def test_load_yaml_invalid_class_path(tmp_path):
    """Test error for invalid class path."""
    yaml_content = """
agents:
  - name: bad_class
    class: InvalidClassName
"""
    yaml_file = tmp_path / "bad_class.yaml"
    yaml_file.write_text(yaml_content)

    registry = AgentRegistry()

    with pytest.raises(ValueError, match="must be fully qualified"):
        registry.load_from_yaml(yaml_file)


def test_load_yaml_nonexistent_module(tmp_path):
    """Test error for non-existent module."""
    yaml_content = """
agents:
  - name: bad_module
    class: nonexistent.module.ClassName
"""
    yaml_file = tmp_path / "bad_module.yaml"
    yaml_file.write_text(yaml_content)

    registry = AgentRegistry()

    with pytest.raises(ImportError, match="Cannot import module"):
        registry.load_from_yaml(yaml_file)


def test_load_yaml_nonexistent_class(tmp_path):
    """Test error for non-existent class in valid module."""
    yaml_content = """
agents:
  - name: bad_class_name
    class: app.agents.research_agent.NonExistentClass
"""
    yaml_file = tmp_path / "bad_class_name.yaml"
    yaml_file.write_text(yaml_content)

    registry = AgentRegistry()

    with pytest.raises(ImportError, match="Class .* not found"):
        registry.load_from_yaml(yaml_file)


def test_load_yaml_with_defaults(tmp_path):
    """Test loading YAML with default values for optional fields."""
    yaml_content = """
agents:
  - name: minimal_agent
    class: app.agents.research_agent.ResearchAgent
"""
    yaml_file = tmp_path / "minimal.yaml"
    yaml_file.write_text(yaml_content)

    registry = AgentRegistry()
    registry.load_from_yaml(yaml_file)

    metadata = registry.get_metadata("minimal_agent")
    assert metadata.config == {}
    assert metadata.tools == []
    assert metadata.description == ""


def test_yaml_and_programmatic_registration(tmp_path):
    """Test that YAML loading and programmatic registration can coexist."""
    yaml_content = """
agents:
  - name: yaml_agent
    class: app.agents.research_agent.ResearchAgent
"""
    yaml_file = tmp_path / "agents.yaml"
    yaml_file.write_text(yaml_content)

    registry = AgentRegistry()

    # Load from YAML
    registry.load_from_yaml(yaml_file)

    # Register programmatically
    registry.register("manual_agent", AssessmentAgent)

    # Both should exist
    assert registry.has("yaml_agent")
    assert registry.has("manual_agent")
    assert len(registry.list_all()) == 2


def test_load_production_config():
    """Test loading the actual production config/agents.yaml file."""
    config_file = Path("config/agents.yaml")

    if not config_file.exists():
        pytest.skip("Production config file not found")

    registry = AgentRegistry()
    registry.load_from_yaml(config_file)

    # Should have loaded research and assessment agents
    assert registry.has("research")
    assert registry.has("assessment")
"""Test script to verify Agent Registry Phase 1 implementation.

This script runs the "Definition of Done" code from the requirements
to verify that the registry works as expected.
"""

from app.agents.assessment_agent import AssessmentAgent
from app.agents.registry import AgentRegistry
from app.agents.research_agent import ResearchAgent

# Create registry
registry = AgentRegistry()

# Register agents
registry.register(
    "research",
    ResearchAgent,
    config={"model": "gpt-4-turbo", "temperature": 0.7},
    tools=["web_search"],
    description="Gathers information from web sources",
)

registry.register(
    "assessment",
    AssessmentAgent,
    config={"model": "gpt-4-turbo", "temperature": 0.3},
    tools=["fact_checker"],
    description="Assesses research quality",
)

# List all
print("Registered agents:", registry.list_all())  # ['research', 'assessment']
assert len(registry.list_all()) == 2

# Get singleton
agent1 = registry.get("research")
agent2 = registry.get("research")
assert agent1 is agent2  # Same instance
print(" Singleton pattern verified")

# Create new
agent3 = registry.create_new("research", temperature=0.9)
assert agent3 is not agent1  # Different instance
print(" Fresh instance creation verified")

# Get metadata
metadata = registry.get_metadata("research")
assert "web_search" in metadata.tools
assert metadata.description == "Gathers information from web sources"
print(" Metadata retrieval verified")

print("\n Phase 1 Complete!")
print(f"   - {len(registry.list_all())} agents registered")
print("   - Singleton pattern working")
print("   - Fresh instance creation working")
print("   - Metadata retrieval working")
